[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction: The Effect of Geographical Awareness Dispersion Through Technology on American Pop Culture",
    "section": "",
    "text": "Awareness of other parts of the world has developed as the prevalence of daily technological usage continues to grow. While previously displayed primarily through specific cable network channels, discovering new travel and international cuisine content on social media applications and streaming platforms is now an expectation as a consumer of technology. The phenomena of technology being a catalyst for the noticeable evolution of accurate geographical awareness holds several implications for readers.//\nWith the growth in international colloquial adoption, has come the subsequent growth in intercultural appreciation. One paper pointed out how international students, “can also easily teach their English-speaking classmates about their own native language through the translators” (MacFarlane (2011)). Translators are a direct technological tool for closing cultural gaps. As sharing cultural information with those around you is a common way people establish connections, technological advances help to further deepen those connections.//\nWhereas from an external point of view, technological advances that keep the communicative diversity of their audience in mind appear to offer better opportunities for language gaps to be minimized. A study examining Chinese students in China who attempted to learn colloquialisms through paper content or WeChat observed mobile devices offer, “functions (such as instant chatting, dictionary, or translation) to help learners resolve questions encountered in self-learning”(Yang and Yin (2018)). This observation refers to the vast accessibility many technological platforms provide. One direct reflection of this would be streaming platform’s multilingual closed captioning providing viewers with more international– an otherwise excluded– content at their disposal.//\nI want to explore this phenomena as it pertains to popular culture in the United States of America. I hypothesize the rise of international content being consumed through streaming platforms and the rise in the incorporation of international colloquialisms will yield a positive correlation. Through my research, I hope to address some topics such as:// - What is currently the most influential streaming service on the specific jargon adoption? - Which regions seem to currently have the largest influence on colloquialism in American popular culture? - How can my findings be utilized to further improve inclusivity through more accurate and widespread adoption?//\nWith that said, the prioritization of international awareness when incorporating methods, such as natural language processing, is already evident. For example, many chatbots are able to communicate in Nigerian pidgin once requested. Also, Apple’s renowned digital assistant, Siri, has the ability to define and use colloquialisms from the United Kingdom upon request. Moreover, through this research, I hope to shed light on the current effects of such tools and highlight focal points for precise innovation.//\n\nQuestions:\n\nExamples of American pop culture colloquialisms? OR Example of adopted pop culture colloquialisms?\nWhat regions in the United States have more of an effect on shared national colloquialisms?\nWhat regions globally have more of an effect on shared national colloquialisms (limited to seven continents)?\nIs there a specific age group who is more impressionable to imported jargon?\nIs there a specific region who is more impressionable to imported jargon?\nLargest language dispersion channel (like Netflix, Hulu, Amazon Prime, Apple TV)?\nIs genre influential on how adaptable the terms are?\nDoes positive or negative content have more of an effect on adapted jargon?\nPractical applications for encouraging international colloquialisms dispersal?\nHow can these applications adapt to the growth of international colloquialisms in America?\n\n\n\n\n\n\nReferences\n\nMacFarlane, Nicole. 2011. “Different Language, Different World: Bridging Gaps in Worlds Through Technology.” Antistasis 1 (2).\n\n\nYang, Jia, and Chengxu Yin. 2018. “Learning Chinese Colloquialisms Through Mobile Technology.” Journal of Technology and Chinese Language Teaching 9 (1): 35–47."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Ofure Udabor is a current first- year student in the Data Science and Analytics program at Georgetown University. She received her B.A. in Psychology and Economics from the University of Texas at Austin in 2022, where she her research focused on sociological effects of psychological and economic phenomena. During her undergraduate studies, she interned as a Data Analyst for an ecommerce brand and worked as a manager for a healthcare company while receiving academic accolades– such as University Honors– and merit scholarships– such as the Walter B. Smith Jr. Undergraduate Scholarship. Now as a graduate student, she hopes to gain a deep understanding of data science and how it can be leveraged in marketing to gain stronger insights from consumer behavior."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nGeorgetown University | Washington, DC\nM.S. in Data Science and Analytics | Aug 2023 - Present\nUniveristy of Texas at Austin | Austin, Texas\nB.A in Economics; B.A. in Psychology | Aug 2018 - May 2022"
  },
  {
    "objectID": "about.html#academic-interests",
    "href": "about.html#academic-interests",
    "title": "About",
    "section": "Academic Interests",
    "text": "Academic Interests\n\nNatural Language Processing:\nAs pertains to marketing\nMarketing Analytics:\ngo in depth\nDecision Analysis:\ngo in depth"
  },
  {
    "objectID": "datacleaning.html",
    "href": "datacleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "import requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nappledf = pd.read_csv(\"../websitedata/appletopsongsinternational2023.csv\")\nprint(appledf.shape)\nprint(type(appledf))\n\nappledf.dropna\n\nappledf = pd.melt(appledf, id_vars=['Pos', 'P+', 'Artist and Title', 'Days', 'Pk', '(x?)', 'Pts', 'Pts+', 'TPts'], \n                   var_name='country', value_name='country_rank').dropna()\n\nprint(appledf)\n\n(199, 69)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n       Pos  P+                     Artist and Title  Days  Pk   (x?)    Pts  \\\n0        1   =        Doja Cat - Paint The Town Red    68   1  (x45)  16920   \n1        2   =                  Tate McRae - greedy    27   2   (x9)  14619   \n2        3   1                        Drake - IDGAF     6   3   (x1)  13364   \n3        4  -1         Drake - First Person Shooter     6   3   (x2)  12624   \n4        5   =               Drake - Virginia Beach     6   2   (x2)  12359   \n...    ...  ..                                  ...   ...  ..    ...    ...   \n11747    7   1          Taylor Swift - Cruel Summer   245   1  (x13)  10731   \n11770   30   2  Billie Eilish - What Was I Made For    90   2  (x10)   5425   \n11773   33   2             Taylor Swift - Anti-Hero   356   1  (x50)   5108   \n11802   62   1                 Taylor Swift - Lover   412   3  (x13)   2739   \n11810   70   1                      SZA - Kill Bill   307   1   (x3)   2487   \n\n       Pts+   TPts country  country_rank  \n0       -88  0.999      US          29.0  \n1        14  0.317      US          44.0  \n2        21  0.075      US           1.0  \n3      -724  0.076      US           2.0  \n4      -627  0.079      US           3.0  \n...     ...    ...     ...           ...  \n11747   216  2.271      VN          28.0  \n11770   186  0.946      VN         166.0  \n11773   177  3.878      VN         146.0  \n11802    24  2.169      VN         116.0  \n11810   187  2.601      VN         104.0  \n\n[836 rows x 11 columns]\n\n\n\n![R Equivalent](images/rEquivalent.png)\n\n/bin/bash: -c: line 1: syntax error near unexpected token `('\n/bin/bash: -c: line 1: `[R Equivalent](images/rEquivalent.png)'"
  },
  {
    "objectID": "datagathering.html",
    "href": "datagathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "My original data sources include datasets from kworb.net and dataworld.com"
  },
  {
    "objectID": "datacleaning.html#datasets",
    "href": "datacleaning.html#datasets",
    "title": "Data Cleaning",
    "section": "",
    "text": "import requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nappledf = pd.read_csv(\"../websitedata/appletopsongsinternational2023.csv\")\nprint(appledf.shape)\nprint(type(appledf))\n\nappledf.dropna\n\nappledf = pd.melt(appledf, id_vars=['Pos', 'P+', 'Artist and Title', 'Days', 'Pk', '(x?)', 'Pts', 'Pts+', 'TPts'], \n                   var_name='country', value_name='country_rank').dropna()\n\nprint(appledf)\n\n(199, 69)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n       Pos  P+                     Artist and Title  Days  Pk   (x?)    Pts  \\\n0        1   =        Doja Cat - Paint The Town Red    68   1  (x45)  16920   \n1        2   =                  Tate McRae - greedy    27   2   (x9)  14619   \n2        3   1                        Drake - IDGAF     6   3   (x1)  13364   \n3        4  -1         Drake - First Person Shooter     6   3   (x2)  12624   \n4        5   =               Drake - Virginia Beach     6   2   (x2)  12359   \n...    ...  ..                                  ...   ...  ..    ...    ...   \n11747    7   1          Taylor Swift - Cruel Summer   245   1  (x13)  10731   \n11770   30   2  Billie Eilish - What Was I Made For    90   2  (x10)   5425   \n11773   33   2             Taylor Swift - Anti-Hero   356   1  (x50)   5108   \n11802   62   1                 Taylor Swift - Lover   412   3  (x13)   2739   \n11810   70   1                      SZA - Kill Bill   307   1   (x3)   2487   \n\n       Pts+   TPts country  country_rank  \n0       -88  0.999      US          29.0  \n1        14  0.317      US          44.0  \n2        21  0.075      US           1.0  \n3      -724  0.076      US           2.0  \n4      -627  0.079      US           3.0  \n...     ...    ...     ...           ...  \n11747   216  2.271      VN          28.0  \n11770   186  0.946      VN         166.0  \n11773   177  3.878      VN         146.0  \n11802    24  2.169      VN         116.0  \n11810   187  2.601      VN         104.0  \n\n[836 rows x 11 columns]\n\n\n\n![R Equivalent](images/rEquivalent.png)\n\n/bin/bash: -c: line 1: syntax error near unexpected token `('\n/bin/bash: -c: line 1: `[R Equivalent](images/rEquivalent.png)'"
  },
  {
    "objectID": "datacleaning.html#api",
    "href": "datacleaning.html#api",
    "title": "Data Cleaning",
    "section": "API",
    "text": "API\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n##love island, burna boy, peaky blinders\n\nLove Island\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=3\nverbose=True\n\nAPI_KEY = 'cf4c8b44470b4d078e5560aab750d39e'\nTOPIC='love island'\n\nURLpost = {'apiKey': 'cf4c8b44470b4d078e5560aab750d39e',\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\nprint(baseURL)\n\n#GET DATA FROM API\nresponse = requests.get(baseURL, URLpost) #request data from the server\n# print(response.url);  \nresponse = response.json() #extract txt data from request into json\n\n# PRETTY PRINT\n# https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\nprint(json.dumps(response, indent=2))\n\n# #GET TIMESTAMP FOR PULL REQUEST\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n# SAVE TO FILE \nwith open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n    json.dump(response, outfile, indent=4)\n\n\ndef string_cleaner(input_string): #replace 'input_string' with 'avocado'\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\narticle_list=response['articles']   #list of dictionaries for each article\narticle_keys=article_list[0].keys()\nprint(\"AVAILABLE KEYS:\")\nprint(article_keys)\nindex=0\ncleaned_data=[];  \nfor article in article_list:\n    tmp=[]\n    if(verbose):\n        print(\"#------------------------------------------\")\n        print(\"#\",index)\n        print(\"#------------------------------------------\")\n\n    for key in article_keys:\n        if(verbose):\n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='description'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='content'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n            #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n                print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data.append(tmp)\n    index+=1\n\n\ndf = pd.DataFrame(cleaned_data, columns= ['source', 'author', 'title', 'description', 'url', 'content'])\ndf_new = df[['title', 'description']]\ndf_new.to_csv('cleanedloveisland.csv', index=False) #,index_label=['title','src','author','date','description'])\ndf_new.head()\n\nAfrobeats\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=3\nverbose=True\n\nAPI_KEY = 'cf4c8b44470b4d078e5560aab750d39e'\nTOPIC='afrobeats'\n\nURLpost = {'apiKey': 'cf4c8b44470b4d078e5560aab750d39e',\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\nprint(baseURL)\n\n#GET DATA FROM API\nresponse = requests.get(baseURL, URLpost) #request data from the server\n# print(response.url);  \nresponse = response.json() #extract txt data from request into json\n\n# PRETTY PRINT\n# https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\nprint(json.dumps(response, indent=2))\n\n# #GET TIMESTAMP FOR PULL REQUEST\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n# SAVE TO FILE \nwith open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n    json.dump(response, outfile, indent=4)\n\n\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\narticle_list=response['articles']   #list of dictionaries for each article\narticle_keys=article_list[0].keys()\nprint(\"AVAILABLE KEYS:\")\nprint(article_keys)\nindex=0\ncleaned_data=[];  \nfor article in article_list:\n    tmp=[]\n    if(verbose):\n        print(\"#------------------------------------------\")\n        print(\"#\",index)\n        print(\"#------------------------------------------\")\n\n    for key in article_keys:\n        if(verbose):\n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='description'):            \n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='content'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n            #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n                print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data.append(tmp)\n    index+=1\n\n\ndf = pd.DataFrame(cleaned_data, columns= ['source', 'author', 'title', 'description', 'url', 'content'])\ndf_newer = df[['title', 'description']]\ndf_newer.to_csv('cleanedafrobeats.csv', index=False) #,index_label=['title','src','author','date','description'])\n\nPeaky Blinders\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=3\nverbose=True\n\nAPI_KEY = 'cf4c8b44470b4d078e5560aab750d39e'\nTOPIC='peaky blinders'\n\nURLpost = {'apiKey': 'cf4c8b44470b4d078e5560aab750d39e',\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\n\nprint(baseURL)\n\n#GET DATA FROM API\nresponse = requests.get(baseURL, URLpost) #request data from the server\n# print(response.url);  \nresponse = response.json() #extract txt data from request into json\n\n# PRETTY PRINT\n# https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\nprint(json.dumps(response, indent=2))\n\n# #GET TIMESTAMP FOR PULL REQUEST\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n# SAVE TO FILE \nwith open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n    json.dump(response, outfile, indent=4)\n\n\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\narticle_list=response['articles']   #list of dictionaries for each article\narticle_keys=article_list[0].keys()\nprint(\"AVAILABLE KEYS:\")\nprint(article_keys)\nindex=0\ncleaned_data=[];  \nfor article in article_list:\n    tmp=[]\n    if(verbose):\n        print(\"#------------------------------------------\")\n        print(\"#\",index)\n        print(\"#------------------------------------------\")\n\n    for key in article_keys:\n        if(verbose):\n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='description'):\n           tmp.append(string_cleaner(article[key]))\n\n        if(key=='content'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n            #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n                print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data.append(tmp)\n    index+=1\n\n\ndf = pd.DataFrame(cleaned_data, columns= ['source', 'author', 'title', 'description', 'url', 'content'])\ndf_newest = df[['title', 'description']]\ndf_newest.to_csv('cleanedpeaky.csv', index=False) #,index_label=['title','src','author','date','description'])\n\nCleaned to a Word Cloud\n\n#df -&gt; list-&gt; string\n\nloveislandtitle = df_new['title'].tolist()\nloveislanddesc = df_new['description'].tolist()\nloveislandtitle.append(loveislanddesc)\nloveislandtitle = ' '.join([str(elem) for elem in loveislandtitle])\n\n\n# MODIFIED FROM \n# https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ntext= loveislandtitle\ngenerate_word_cloud(text)\n\n\n#df -&gt; list-&gt; string\n\nafrobeatstitle = df_newer['title'].tolist()\nafrobeatsdesc = df_newer['description'].tolist()\nafrobeatstitle.append(afrobeatsdesc)\nafrobeatstitle = ' '.join([str(elem) for elem in afrobeatstitle])\n\n\n# MODIFIED FROM \n# https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ntext= afrobeatstitle\ngenerate_word_cloud(text)\n\n\n#df -&gt; list-&gt; string\n\npeakyblinderstitle = df_newest['title'].tolist()\npeakyblindersdesc = df_newest['description'].tolist()\npeakyblinderstitle.append(peakyblindersdesc)\npeakyblinderstitle = ' '.join([str(elem) for elem in peakyblinderstitle])\n\n\n# MODIFIED FROM \n# https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ntext= peakyblinderstitle\ngenerate_word_cloud(text)"
  },
  {
    "objectID": "datagathering.html#data-sources",
    "href": "datagathering.html#data-sources",
    "title": "Data Gathering",
    "section": "",
    "text": "My original data sources include datasets from kworb.net and dataworld.com"
  },
  {
    "objectID": "datagathering.html#data-links",
    "href": "datagathering.html#data-links",
    "title": "Data Gathering",
    "section": "Data Links",
    "text": "Data Links"
  },
  {
    "objectID": "datagathering.html#data-sites",
    "href": "datagathering.html#data-sites",
    "title": "Data Gathering",
    "section": "Data Sites",
    "text": "Data Sites"
  }
]