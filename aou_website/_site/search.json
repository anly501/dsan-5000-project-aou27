[
  {
    "objectID": "naivebayes.html",
    "href": "naivebayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Provide an brief overview of Naive Bayes classification and how it works. Don’t go into too much detail, assume the audience is familiar with math and statistics but is not familiar with Naive Bayes. Explain the probabilistic nature of Naive Bayes and its Bayes’ theorem foundation. Clearly define the objectives of what you are trying to do. Explain what you aim to achieve through Naive Bayes classification. Describe different variants of Naive Bayes, such as Gaussian, Multinomial, and Bernoulli Naive Bayes, and explain when to use each"
  },
  {
    "objectID": "naivebayes.html#introduction-to-naive-bayes",
    "href": "naivebayes.html#introduction-to-naive-bayes",
    "title": "Naive Bayes",
    "section": "",
    "text": "Provide an brief overview of Naive Bayes classification and how it works. Don’t go into too much detail, assume the audience is familiar with math and statistics but is not familiar with Naive Bayes. Explain the probabilistic nature of Naive Bayes and its Bayes’ theorem foundation. Clearly define the objectives of what you are trying to do. Explain what you aim to achieve through Naive Bayes classification. Describe different variants of Naive Bayes, such as Gaussian, Multinomial, and Bernoulli Naive Bayes, and explain when to use each"
  },
  {
    "objectID": "naivebayes.html#data-prep",
    "href": "naivebayes.html#data-prep",
    "title": "Naive Bayes",
    "section": "Data Prep",
    "text": "Data Prep\nYour goal when using NB is to build (train) a classification model and then use it to make predictions on your test data. Therefore, after you prepare and clean your data, you will need to separate it into TRAINING, VALIDATION, and TESTING sets. Explain how and why the dataset is split into training and testing sets."
  },
  {
    "objectID": "naivebayes.html#feature-selection",
    "href": "naivebayes.html#feature-selection",
    "title": "Naive Bayes",
    "section": "Feature Selection",
    "text": "Feature Selection\nYou need to use either R or Python to code Naive Bayes (NB) as a classification model for your data, we will use NB as a wrapper for feature selection. While not required, if you want to take it further, you can try this in both R and Python.\nObjective: The primary objective of the Feature Selection component in this project is to identify and choose the most relevant and informative features (variables or attributes) from the dataset, for the given task. Effective feature selection can improve the model’s performance, reduce overfitting, and enhance the interpretability of the results.\nInstructions: Generalize and apply the code in the lab assignment and lab-demonstration called “Feature selection with text data” to the text and record data you have collected for your project. This code demonstrates feature selection for a text classification task, so map the task onto your projects dataset."
  },
  {
    "objectID": "naivebayes.html#results--redocrd-data",
    "href": "naivebayes.html#results--redocrd-data",
    "title": "Naive Bayes",
    "section": "Results- Redocrd Data",
    "text": "Results- Redocrd Data\nUsing your optimal feature set from the previous section, fit a final “optimal” NB model for your Record data. Report and comment on the findings. It is required that you create code, appropriate visualizations, result summaries, confusion matrices, etc Describe how the trained model is tested on the testing dataset. Discuss the evaluation metrics used to assess the performance of the Naive Bayes classifier (e.g., accuracy, precision, recall, F1-score). Discuss the concepts of overfitting and under-fitting and whether your model is doing it. Discuss the model’s performance in terms of accuracy and other relevant metrics. Describe how the project findings will be documented and reported, including the format of reports or presentations. e.g. what is the output that you generate. What does the output mean? What does it tell you about your data? Does your model do a good job of predicting your test data? Include and discuss relevant visualizations, results, the confusion matrices, etc . Create and include a minimum of three visualizations for each case (text and record classification). Write a conclusion paragraph interpreting the results. Note, this is not the same as a write-up of technical methodological details."
  },
  {
    "objectID": "naivebayes.html#results--text-data",
    "href": "naivebayes.html#results--text-data",
    "title": "Naive Bayes",
    "section": "Results- Text Data",
    "text": "Results- Text Data\nRepeat HW-3.2.3 but with your text data"
  },
  {
    "objectID": "dataexploration.html",
    "href": "dataexploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "My collected data is meant to display major representative sources of American pop culture. This currently includes headlines from newpapers, top music songs internationally, and the most liked Netflix shows from the past year. As these sources are known to reflect what the general population is interested in, these datasets that seemingly represent America’s favored pasttimes, also present current wave of cultural influence in America. Of these influences, I aim to see how certain streaming services are most influential on the colloquialisms of America.\nBegin by thoroughly understanding the dataset and the problem you are trying to solve. Familiarize yourself with the characteristics of each feature, including their data types (numerical, categorical), potential relationships, and relevance to the project’s objectives.\n\n\n\nCalculate and report basic summary statistics such as mean, median, mode, standard deviation, and variance for numerical variables. For categorical variables, provide frequency distributions and bar charts to visualize data distribution.\nApple:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(appledf.describe())\n\n\n\nApple Summary Statistics\n\n\nNetflix:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(netflixdf.describe())\n\n\n\nNetflix Summary Statistics\n\n\nNews API:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(df_new.describe())\nprint(df_newer.describe())\nprint(df_newest.describe())\n\n\n\nCreate visualizations such as histograms, box plots, scatter plots, and heatmaps to explore the data’s distribution, relationships between variables, and potential patterns or trends. Visualizations can make complex data more interpretable.\n\n\n\nExamine the correlations between variables using correlation matrices, heat-maps, or scatter plots. Identify which variables are positively, negatively, or not correlated, which can guide further analysis.\nApple:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(appledf.corr())\nNetflix:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(netflixdf.corr())\nNews API:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(df_new.corr())\nprint(df_newer.corr())\nprint(df_newest.corr())\n\n\n\nBased on your initial observations from the data, refine your hypotheses and research questions.\n\n\n\nIf applicable, group or segment the data based on relevant criteria to uncover insights within specific subgroups.\n\n\n\nDetect and investigate outliers that may indicate data quality issues or reveal interesting anomalies in the dataset.\n\n\n\nSummarize the key findings, insights, and patterns discovered during the EDA phase. Present these findings in a clear and organized manner, using textual narration, tables, charts, and narrative descriptions.\n\n\n\nMention and discuss the software tools you will use for EDA, such as Python with libraries like Pandas, Matplotlib, Seaborn, or R with ggplot2, depending on your preference and project requirements."
  },
  {
    "objectID": "dataexploration.html#data-understanding",
    "href": "dataexploration.html#data-understanding",
    "title": "Data Exploration",
    "section": "",
    "text": "My collected data is meant to display major representative sources of American pop culture. This currently includes headlines from newpapers, top music songs internationally, and the most liked Netflix shows from the past year. As these sources are known to reflect what the general population is interested in, these datasets that seemingly represent America’s favored pasttimes, also present current wave of cultural influence in America. Of these influences, I aim to see how certain streaming services are most influential on the colloquialisms of America.\nBegin by thoroughly understanding the dataset and the problem you are trying to solve. Familiarize yourself with the characteristics of each feature, including their data types (numerical, categorical), potential relationships, and relevance to the project’s objectives."
  },
  {
    "objectID": "dataexploration.html#descriptive-statistics",
    "href": "dataexploration.html#descriptive-statistics",
    "title": "Data Exploration",
    "section": "",
    "text": "Calculate and report basic summary statistics such as mean, median, mode, standard deviation, and variance for numerical variables. For categorical variables, provide frequency distributions and bar charts to visualize data distribution.\nApple:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(appledf.describe())\n\n\n\nApple Summary Statistics\n\n\nNetflix:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(netflixdf.describe())\n\n\n\nNetflix Summary Statistics\n\n\nNews API:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(df_new.describe())\nprint(df_newer.describe())\nprint(df_newest.describe())"
  },
  {
    "objectID": "dataexploration.html#data-visualizations",
    "href": "dataexploration.html#data-visualizations",
    "title": "Data Exploration",
    "section": "",
    "text": "Create visualizations such as histograms, box plots, scatter plots, and heatmaps to explore the data’s distribution, relationships between variables, and potential patterns or trends. Visualizations can make complex data more interpretable."
  },
  {
    "objectID": "dataexploration.html#correlation-analysis",
    "href": "dataexploration.html#correlation-analysis",
    "title": "Data Exploration",
    "section": "",
    "text": "Examine the correlations between variables using correlation matrices, heat-maps, or scatter plots. Identify which variables are positively, negatively, or not correlated, which can guide further analysis.\nApple:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(appledf.corr())\nNetflix:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(netflixdf.corr())\nNews API:\nimport requests\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\n\nprint(df_new.corr())\nprint(df_newer.corr())\nprint(df_newest.corr())"
  },
  {
    "objectID": "dataexploration.html#hypothesis-generation",
    "href": "dataexploration.html#hypothesis-generation",
    "title": "Data Exploration",
    "section": "",
    "text": "Based on your initial observations from the data, refine your hypotheses and research questions."
  },
  {
    "objectID": "dataexploration.html#data-grouping-and-segmentation",
    "href": "dataexploration.html#data-grouping-and-segmentation",
    "title": "Data Exploration",
    "section": "",
    "text": "If applicable, group or segment the data based on relevant criteria to uncover insights within specific subgroups."
  },
  {
    "objectID": "dataexploration.html#identifying-outliers",
    "href": "dataexploration.html#identifying-outliers",
    "title": "Data Exploration",
    "section": "",
    "text": "Detect and investigate outliers that may indicate data quality issues or reveal interesting anomalies in the dataset."
  },
  {
    "objectID": "dataexploration.html#methods-and-findings",
    "href": "dataexploration.html#methods-and-findings",
    "title": "Data Exploration",
    "section": "",
    "text": "Summarize the key findings, insights, and patterns discovered during the EDA phase. Present these findings in a clear and organized manner, using textual narration, tables, charts, and narrative descriptions."
  },
  {
    "objectID": "dataexploration.html#tools-and-software",
    "href": "dataexploration.html#tools-and-software",
    "title": "Data Exploration",
    "section": "",
    "text": "Mention and discuss the software tools you will use for EDA, such as Python with libraries like Pandas, Matplotlib, Seaborn, or R with ggplot2, depending on your preference and project requirements."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Ofure Udabor is a current first- year student in the Data Science and Analytics program at Georgetown University. She received her B.A. in Psychology and Economics from the University of Texas at Austin in 2022, where she her research focused on sociological effects of psychological and economic phenomena. During her undergraduate studies, she interned as a Data Analyst for an ecommerce brand and worked as a manager for a healthcare company while receiving academic accolades– such as University Honors– and merit scholarships– such as the Walter B. Smith Jr. Undergraduate Scholarship. Now as a graduate student, she hopes to gain a deep understanding of data science and how it can be leveraged in marketing to gain stronger insights from consumer behavior."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nGeorgetown University | Washington, DC\nM.S. in Data Science and Analytics | Aug 2023 - Present\nUniveristy of Texas at Austin | Austin, Texas\nB.A in Economics; B.A. in Psychology | Aug 2018 - May 2022"
  },
  {
    "objectID": "about.html#academic-interests",
    "href": "about.html#academic-interests",
    "title": "About",
    "section": "Academic Interests",
    "text": "Academic Interests\n\nNatural Language Processing:\nAs pertains to marketing\nMarketing Analytics:\ngo in depth\nDecision Analysis:\ngo in depth"
  },
  {
    "objectID": "datacleaning.html",
    "href": "datacleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "import requests\nimport json\nimport re\n#import pycountry\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nappledf = pd.read_csv(\"../websitedata/appletopsongsinternational2023.csv\")\nprint(appledf.shape)\nprint(type(appledf))\n\nappledf.dropna\n\nappledf = pd.melt(appledf, id_vars=[\"Pos\", \"P+\", \"Artist and Title\", \"Days\", \"Pk\", \"(x?)\", \"Pts\", \"Pts+\", \"TPts\"], \n                   var_name=\"country\", value_name=\"country_rank\").dropna()\n\n#print(appledf.isnull().sum())\n#print(appledf.isna().sum())\n\nappledf = appledf.drop(columns=[\"P+\", \"Pts+\", \"TPts\"])\nappledf = appledf.rename(columns={\"Pos\": \"Chart Position\",\"Days\": \"Position Duration\", \"Pk\": \"Peak\", \n                                  \"Pts\": \"Points\", \"country\": \"Country\", \"country_rank\": \"Country Rank\"})\n#appledf = appledf[\"Country\"].apply(lambda x: pycountry.countries.get(alpha_3=x).name \n                                               #if len(x) == 3 else pycountry.countries.get(alpha_2=x).name)\n\n#print(appledf[\"(x?)\"].value_counts())\n\nprint(appledf)\n#appledf.to_csv(\"C:/Users/Owner/Documents/DSAN 5000/HW-02/websitedata/apple_py.csv\")\n\n\n(199, 69)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n       Chart Position                     Artist and Title  Position Duration  \\\n0                   1        Doja Cat - Paint The Town Red                 68   \n1                   2                  Tate McRae - greedy                 27   \n2                   3                        Drake - IDGAF                  6   \n3                   4         Drake - First Person Shooter                  6   \n4                   5               Drake - Virginia Beach                  6   \n...               ...                                  ...                ...   \n11747               7          Taylor Swift - Cruel Summer                245   \n11770              30  Billie Eilish - What Was I Made For                 90   \n11773              33             Taylor Swift - Anti-Hero                356   \n11802              62                 Taylor Swift - Lover                412   \n11810              70                      SZA - Kill Bill                307   \n\n       Peak   (x?)  Points Country  Country Rank  \n0         1  (x45)   16920      US          29.0  \n1         2   (x9)   14619      US          44.0  \n2         3   (x1)   13364      US           1.0  \n3         3   (x2)   12624      US           2.0  \n4         2   (x2)   12359      US           3.0  \n...     ...    ...     ...     ...           ...  \n11747     1  (x13)   10731      VN          28.0  \n11770     2  (x10)    5425      VN         166.0  \n11773     1  (x50)    5108      VN         146.0  \n11802     3  (x13)    2739      VN         116.0  \n11810     1   (x3)    2487      VN         104.0  \n\n[836 rows x 8 columns]"
  },
  {
    "objectID": "datacleaning.html#apple-music-international-rankings",
    "href": "datacleaning.html#apple-music-international-rankings",
    "title": "Data Cleaning",
    "section": "",
    "text": "import requests\nimport json\nimport re\n#import pycountry\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nappledf = pd.read_csv(\"../websitedata/appletopsongsinternational2023.csv\")\nprint(appledf.shape)\nprint(type(appledf))\n\nappledf.dropna\n\nappledf = pd.melt(appledf, id_vars=[\"Pos\", \"P+\", \"Artist and Title\", \"Days\", \"Pk\", \"(x?)\", \"Pts\", \"Pts+\", \"TPts\"], \n                   var_name=\"country\", value_name=\"country_rank\").dropna()\n\n#print(appledf.isnull().sum())\n#print(appledf.isna().sum())\n\nappledf = appledf.drop(columns=[\"P+\", \"Pts+\", \"TPts\"])\nappledf = appledf.rename(columns={\"Pos\": \"Chart Position\",\"Days\": \"Position Duration\", \"Pk\": \"Peak\", \n                                  \"Pts\": \"Points\", \"country\": \"Country\", \"country_rank\": \"Country Rank\"})\n#appledf = appledf[\"Country\"].apply(lambda x: pycountry.countries.get(alpha_3=x).name \n                                               #if len(x) == 3 else pycountry.countries.get(alpha_2=x).name)\n\n#print(appledf[\"(x?)\"].value_counts())\n\nprint(appledf)\n#appledf.to_csv(\"C:/Users/Owner/Documents/DSAN 5000/HW-02/websitedata/apple_py.csv\")\n\n\n(199, 69)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n       Chart Position                     Artist and Title  Position Duration  \\\n0                   1        Doja Cat - Paint The Town Red                 68   \n1                   2                  Tate McRae - greedy                 27   \n2                   3                        Drake - IDGAF                  6   \n3                   4         Drake - First Person Shooter                  6   \n4                   5               Drake - Virginia Beach                  6   \n...               ...                                  ...                ...   \n11747               7          Taylor Swift - Cruel Summer                245   \n11770              30  Billie Eilish - What Was I Made For                 90   \n11773              33             Taylor Swift - Anti-Hero                356   \n11802              62                 Taylor Swift - Lover                412   \n11810              70                      SZA - Kill Bill                307   \n\n       Peak   (x?)  Points Country  Country Rank  \n0         1  (x45)   16920      US          29.0  \n1         2   (x9)   14619      US          44.0  \n2         3   (x1)   13364      US           1.0  \n3         3   (x2)   12624      US           2.0  \n4         2   (x2)   12359      US           3.0  \n...     ...    ...     ...     ...           ...  \n11747     1  (x13)   10731      VN          28.0  \n11770     2  (x10)    5425      VN         166.0  \n11773     1  (x50)    5108      VN         146.0  \n11802     3  (x13)    2739      VN         116.0  \n11810     1   (x3)    2487      VN         104.0  \n\n[836 rows x 8 columns]"
  },
  {
    "objectID": "datacleaning.html#r-equivalent",
    "href": "datacleaning.html#r-equivalent",
    "title": "Data Cleaning",
    "section": "R Equivalent",
    "text": "R Equivalent\n\n\n\nR Equivalent to Apple Data Gathering and Cleaning"
  },
  {
    "objectID": "datacleaning.html#netflix-best-shows-of-2022",
    "href": "datacleaning.html#netflix-best-shows-of-2022",
    "title": "Data Cleaning",
    "section": "Netflix Best Shows of 2022",
    "text": "Netflix Best Shows of 2022\n\nimport requests\nimport json\nimport re\n#import pycountry\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nnetflixdf = pd.read_csv(\"../websitedata/netflixbestshows2022.csv\")\nprint(netflixdf.shape)\nprint(type(netflixdf))\n\nnetflixdf.dropna\n#print(netflixdf.isnull().sum())\n#print(netflixdf.isna().sum())\n\nnetflixdf = netflixdf.rename(columns={\"TITLE\": \"Title\", \"RELEASE_YEAR\": \"Release Year\", \"SCORE\": \"Score\", \n                                      \"NUMBER_OF_VOTES\": \"Votes (Amount)\", \"DURATION\": \"Duration\", \n                                      \"NUMBER_OF_SEASONS\": \"Total Seasons\", \"MAIN_GENRE\": \"Main Genre\", \n                                      \"MAIN_PRODUCTION\": \"Main Production\"})\n\n#netflixdf = netflixdf[\"Main Production\"].apply(lambda x: pycountry.countries.get(alpha_3=x).name \n                                               #if len(x) == 3 else pycountry.countries.get(alpha_2=x).name)\nnetflixdf.sort_values(by = [\"Release Year\"], ascending=False)\n\nnetflixdf = netflixdf.drop(\"Total Seasons\", axis=1)\n\n#netflixdf[\"Votes (Amount)\"] = netflixdf[\"Votes (Amount)\"].apply(lambda x: str(x) if x &lt; 50000 else x)\nnetflixdf[\"Votes (Amount)\"] = netflixdf[\"Votes (Amount)\"].astype(str)\n\n#voters = netflixdf[\"Votes (Amount)\"] &lt; \"50000\"\n#netflixdf.loc[voters, \"Votes (Amount)\"] = \"low five\"\n#netflixdf[\"Votes (Amount)\"] = netflixdf[\"Votes (Amount)\"].astype(categorical)\n\n#voters1 = netflixdf[\"Votes (Amount)\"] &gt;= \"50000\") & (netflixdf[\"Votes (Amount)\"] &lt; \"100000\")\n#netflixdf.loc[voters1, \"Votes (Amount)\"] = \"high five\"\n\n\nprint(netflixdf)\n#netflixdf.to_csv(\"C:/Users/Owner/Documents/DSAN 5000/website-template/HW1_5000/dsan-5000-project-aou27/websitedata\")\n\n#print(netflixdf[\"Score\"].min())\n#print(netflixdf[\"Score\"].max())\n\n#print(netflixdf[\"Votes (Amount)\"].min())\n#print(netflixdf[\"Votes (Amount)\"].max())\n\n(246, 8)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n                          Title  Release Year  Score Votes (Amount)  Duration  \\\n0                  Breaking Bad          2008    9.5       low five        48   \n1    Avatar: The Last Airbender          2005    9.3       low five        24   \n2                    Our Planet          2019    9.3       low five        50   \n3                  Kota Factory          2019    9.3          66985        42   \n4                The Last Dance          2020    9.1       low five        50   \n..                          ...           ...    ...            ...       ...   \n241                 Evil Genius          2018    7.5       low five        48   \n242              13 Reasons Why          2017    7.5       low five        58   \n243                       Lupin          2021    7.5       low five        46   \n244          All of Us Are Dead          2022    7.5       low five        61   \n245     I Am Not Okay with This          2020    7.5          56459        21   \n\n      Main Genre Main Production  \n0          drama              US  \n1          scifi              US  \n2    documentary              GB  \n3          drama              IN  \n4    documentary              US  \n..           ...             ...  \n241        crime              US  \n242        drama              US  \n243        crime              FR  \n244       action              KR  \n245       comedy              US  \n\n[246 rows x 7 columns]"
  },
  {
    "objectID": "datacleaning.html#r-equivalent-1",
    "href": "datacleaning.html#r-equivalent-1",
    "title": "Data Cleaning",
    "section": "R Equivalent",
    "text": "R Equivalent\n\n\n\nR Equivalent to Netflix Data Gathering and Cleaning"
  },
  {
    "objectID": "datacleaning.html#text-api",
    "href": "datacleaning.html#text-api",
    "title": "Data Cleaning",
    "section": "Text API",
    "text": "Text API\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n##love island, burna boy, peaky blinders\n\nLove Island\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=3\nverbose=True\n\nAPI_KEY = 'cf4c8b44470b4d078e5560aab750d39e'\nTOPIC='love island'\n\nURLpost = {'apiKey': 'cf4c8b44470b4d078e5560aab750d39e',\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\nprint(baseURL)\n\n#GET DATA FROM API\nresponse = requests.get(baseURL, URLpost) #request data from the server\n# print(response.url);  \nresponse = response.json() #extract txt data from request into json\n\n# PRETTY PRINT\n# https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\nprint(json.dumps(response, indent=2))\n\n# #GET TIMESTAMP FOR PULL REQUEST\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n# SAVE TO FILE \nwith open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n    json.dump(response, outfile, indent=4)\n\n\ndef string_cleaner(input_string): #replace 'input_string' with 'avocado'\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\narticle_list=response['articles']   #list of dictionaries for each article\narticle_keys=article_list[0].keys()\nprint(\"AVAILABLE KEYS:\")\nprint(article_keys)\nindex=0\ncleaned_data=[];  \nfor article in article_list:\n    tmp=[]\n    if(verbose):\n        print(\"#------------------------------------------\")\n        print(\"#\",index)\n        print(\"#------------------------------------------\")\n\n    for key in article_keys:\n        if(verbose):\n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='description'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='content'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n            #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n                print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data.append(tmp)\n    index+=1\n\n\ndf = pd.DataFrame(cleaned_data, columns= ['source', 'author', 'title', 'description', 'url', 'content'])\ndf_new = df[['title', 'description']]\ndf_new.to_csv('cleanedloveisland.csv', index=False) #,index_label=['title','src','author','date','description'])\ndf_new.head()\n\nAfrobeats\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=3\nverbose=True\n\nAPI_KEY = 'cf4c8b44470b4d078e5560aab750d39e'\nTOPIC='afrobeats'\n\nURLpost = {'apiKey': 'cf4c8b44470b4d078e5560aab750d39e',\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\nprint(baseURL)\n\n#GET DATA FROM API\nresponse = requests.get(baseURL, URLpost) #request data from the server\n# print(response.url);  \nresponse = response.json() #extract txt data from request into json\n\n# PRETTY PRINT\n# https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\nprint(json.dumps(response, indent=2))\n\n# #GET TIMESTAMP FOR PULL REQUEST\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n# SAVE TO FILE \nwith open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n    json.dump(response, outfile, indent=4)\n\n\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\narticle_list=response['articles']   #list of dictionaries for each article\narticle_keys=article_list[0].keys()\nprint(\"AVAILABLE KEYS:\")\nprint(article_keys)\nindex=0\ncleaned_data=[];  \nfor article in article_list:\n    tmp=[]\n    if(verbose):\n        print(\"#------------------------------------------\")\n        print(\"#\",index)\n        print(\"#------------------------------------------\")\n\n    for key in article_keys:\n        if(verbose):\n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='description'):            \n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='content'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n            #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n                print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data.append(tmp)\n    index+=1\n\n\ndf = pd.DataFrame(cleaned_data, columns= ['source', 'author', 'title', 'description', 'url', 'content'])\ndf_newer = df[['title', 'description']]\ndf_newer.to_csv('cleanedafrobeats.csv', index=False) #,index_label=['title','src','author','date','description'])\n\nPeaky Blinders\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=3\nverbose=True\n\nAPI_KEY = 'cf4c8b44470b4d078e5560aab750d39e'\nTOPIC='peaky blinders'\n\nURLpost = {'apiKey': 'cf4c8b44470b4d078e5560aab750d39e',\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\n\nprint(baseURL)\n\n#GET DATA FROM API\nresponse = requests.get(baseURL, URLpost) #request data from the server\n# print(response.url);  \nresponse = response.json() #extract txt data from request into json\n\n# PRETTY PRINT\n# https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\nprint(json.dumps(response, indent=2))\n\n# #GET TIMESTAMP FOR PULL REQUEST\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n# SAVE TO FILE \nwith open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n    json.dump(response, outfile, indent=4)\n\n\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\narticle_list=response['articles']   #list of dictionaries for each article\narticle_keys=article_list[0].keys()\nprint(\"AVAILABLE KEYS:\")\nprint(article_keys)\nindex=0\ncleaned_data=[];  \nfor article in article_list:\n    tmp=[]\n    if(verbose):\n        print(\"#------------------------------------------\")\n        print(\"#\",index)\n        print(\"#------------------------------------------\")\n\n    for key in article_keys:\n        if(verbose):\n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='description'):\n           tmp.append(string_cleaner(article[key]))\n\n        if(key=='content'):\n            tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n            #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n                print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data.append(tmp)\n    index+=1\n\n\ndf = pd.DataFrame(cleaned_data, columns= ['source', 'author', 'title', 'description', 'url', 'content'])\ndf_newest = df[['title', 'description']]\ndf_newest.to_csv('cleanedpeaky.csv', index=False) #,index_label=['title','src','author','date','description'])\n\nCleaned to a Word Cloud\n\n#df -&gt; list-&gt; string\n\nloveislandtitle = df_new['title'].tolist()\nloveislanddesc = df_new['description'].tolist()\nloveislandtitle.append(loveislanddesc)\nloveislandtitle = ' '.join([str(elem) for elem in loveislandtitle])\n\n\n# MODIFIED FROM \n# https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ntext= loveislandtitle\ngenerate_word_cloud(text)\n\n\n#df -&gt; list-&gt; string\n\nafrobeatstitle = df_newer['title'].tolist()\nafrobeatsdesc = df_newer['description'].tolist()\nafrobeatstitle.append(afrobeatsdesc)\nafrobeatstitle = ' '.join([str(elem) for elem in afrobeatstitle])\n\n\n# MODIFIED FROM \n# https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ntext= afrobeatstitle\ngenerate_word_cloud(text)\n\n\n#df -&gt; list-&gt; string\n\npeakyblinderstitle = df_newest['title'].tolist()\npeakyblindersdesc = df_newest['description'].tolist()\npeakyblinderstitle.append(peakyblindersdesc)\npeakyblinderstitle = ' '.join([str(elem) for elem in peakyblinderstitle])\n\n\n# MODIFIED FROM \n# https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ntext= peakyblinderstitle\ngenerate_word_cloud(text)"
  },
  {
    "objectID": "datagathering.html",
    "href": "datagathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "My original data sources include datasets from kworb.net and dataworld.com that document rankings as of October 12, 2023."
  },
  {
    "objectID": "datagathering.html#data-sources",
    "href": "datagathering.html#data-sources",
    "title": "Data Gathering",
    "section": "",
    "text": "My original data sources include datasets from kworb.net and dataworld.com that document rankings as of October 12, 2023."
  },
  {
    "objectID": "datagathering.html#data-sites",
    "href": "datagathering.html#data-sites",
    "title": "Data Gathering",
    "section": "Data Sites",
    "text": "Data Sites"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction: The Effect of Geographical Awareness Dispersion Through Technology on American Pop Culture",
    "section": "",
    "text": "Awareness of other parts of the world has developed as the prevalence of daily technological usage continues to grow. While previously displayed primarily through specific cable network channels, discovering new travel and international cuisine content on social media applications and streaming platforms is now an expectation as a consumer of technology. The phenomena of technology being a catalyst for the noticeable evolution of accurate geographical awareness holds several implications for readers.//\nWith the growth in international colloquial adoption, has come the subsequent growth in intercultural appreciation. One paper pointed out how international students, “can also easily teach their English-speaking classmates about their own native language through the translators” (MacFarlane (2011)). Translators are a direct technological tool for closing cultural gaps. As sharing cultural information with those around you is a common way people establish connections, technological advances help to further deepen those connections.//\nWhereas from an external point of view, technological advances that keep the communicative diversity of their audience in mind appear to offer better opportunities for language gaps to be minimized. A study examining Chinese students in China who attempted to learn colloquialisms through paper content or WeChat observed mobile devices offer, “functions (such as instant chatting, dictionary, or translation) to help learners resolve questions encountered in self-learning”(Yang and Yin (2018)). This observation refers to the vast accessibility many technological platforms provide. One direct reflection of this would be streaming platform’s multilingual closed captioning providing viewers with more international– an otherwise excluded– content at their disposal.//\nI want to explore this phenomena as it pertains to popular culture in the United States of America. I hypothesize the rise of international content being consumed through streaming platforms and the rise in the incorporation of international colloquialisms will yield a positive correlation. Through my research, I hope to address some topics such as:// - What is currently the most influential streaming service on the specific jargon adoption? - Which regions seem to currently have the largest influence on colloquialism in American popular culture? - How can my findings be utilized to further improve inclusivity through more accurate and widespread adoption?//\nWith that said, the prioritization of international awareness when incorporating methods, such as natural language processing, is already evident. For example, many chatbots are able to communicate in Nigerian pidgin once requested. Also, Apple’s renowned digital assistant, Siri, has the ability to define and use colloquialisms from the United Kingdom upon request. Moreover, through this research, I hope to shed light on the current effects of such tools and highlight focal points for precise innovation.//\n\nQuestions:\n\nExamples of American pop culture colloquialisms? OR Example of adopted pop culture colloquialisms?\nWhat regions in the United States have more of an effect on shared national colloquialisms?\nWhat regions globally have more of an effect on shared national colloquialisms (limited to seven continents)?\nIs there a specific age group who is more impressionable to imported jargon?\nIs there a specific region who is more impressionable to imported jargon?\nLargest language dispersion channel (like Netflix, Hulu, Amazon Prime, Apple TV)?\nIs genre influential on how adaptable the terms are?\nDoes positive or negative content have more of an effect on adapted jargon?\nPractical applications for encouraging international colloquialisms dispersal?\nHow can these applications adapt to the growth of international colloquialisms in America?\n\n\n\n\n\n\nReferences\n\nMacFarlane, Nicole. 2011. “Different Language, Different World: Bridging Gaps in Worlds Through Technology.” Antistasis 1 (2).\n\n\nYang, Jia, and Chengxu Yin. 2018. “Learning Chinese Colloquialisms Through Mobile Technology.” Journal of Technology and Chinese Language Teaching 9 (1): 35–47."
  },
  {
    "objectID": "naivebayes.html#results--record-data",
    "href": "naivebayes.html#results--record-data",
    "title": "Naive Bayes",
    "section": "Results- Record Data",
    "text": "Results- Record Data\nUsing your optimal feature set from the previous section, fit a final “optimal” NB model for your Record data. Report and comment on the findings. It is required that you create code, appropriate visualizations, result summaries, confusion matrices, etc Describe how the trained model is tested on the testing dataset. Discuss the evaluation metrics used to assess the performance of the Naive Bayes classifier (e.g., accuracy, precision, recall, F1-score). Discuss the concepts of overfitting and under-fitting and whether your model is doing it. Discuss the model’s performance in terms of accuracy and other relevant metrics. Describe how the project findings will be documented and reported, including the format of reports or presentations. e.g. what is the output that you generate. What does the output mean? What does it tell you about your data? Does your model do a good job of predicting your test data? Include and discuss relevant visualizations, results, the confusion matrices, etc . Create and include a minimum of three visualizations for each case (text and record classification). Write a conclusion paragraph interpreting the results. Note, this is not the same as a write-up of technical methodological details."
  }
]