{
  "cells": [
    {
      "cell_type": "raw",
      "id": "2c4ad2b4",
      "metadata": {},
      "source": [
        "---\n",
        "title: Decision Trees\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision Trees is a supervised learning method. This algorithm is constructed of rules that are meant to determine the possible outcome value of a target variable. Like a tree, there is a “root” node, which is simply the initial node of prediction. The nodes that follow, which are called “internal nodes”, are intermediary points of the decision process of the tree. Finally, The endpoints of the nodes are called “leaf nodes”, which house the predicted outcomes of the target variable. \n",
        "\n",
        "There are few different ways for the tree to develop. The decision tree can have a binary split (the parent node is split into two “child nodes”) or a multi- way split (the parent node is split into more than two “child nodes”). Also, the decision tree can have either a continuous target variable or a categorical target variable. The tree with the continuous target variable (quantitative) is referred to as the “Regression Tree”. Whereas the decision tree with the categorical target variable (qualitative) is referred to as the “Classification Tree”.\n",
        "\n",
        "To reduce the variance during such a decision process, Bagging can be used. Bagging, also known as, “Boostrap Aggregation”, is the process of taking many samples from one dataset, training on the desired number of the samples collected, and take an average of the predictions. This method is commonly used on decision trees in hopes of the outcome being more representative of the ground truth. This method, when done on multiple decision trees, is now establishing a “Random Forest”.\n",
        "\n",
        "As decision trees represent one tree- structured process, Random Forests represent multiple. Instead of landing on a chosen outcome at the end of one true, the Random Forests algorithm implements “majority rules” by taking on the outcome that is predicted the most. This majority rules guideline is used for classification models in Random Forests. Whereas in a situation where the target variable is continuous, thus it is a regression model, the Random Forest takes the average of the bagging process to compute an outcome value.\n",
        "\n",
        "Please refer to my \"Regression\" tab to view the decision tree algorithm applied to my chosen dataset.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
