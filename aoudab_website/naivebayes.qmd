# Naive Bayes


## Introduction to Naive Bayes

Provide an brief overview of Naive Bayes classification and how it works.
Don’t go into too much detail, assume the audience is familiar with math and statistics but is not familiar with Naive Bayes.
Explain the probabilistic nature of Naive Bayes and its Bayes’ theorem foundation.
Clearly define the objectives of what you are trying to do.
Explain what you aim to achieve through Naive Bayes classification.
Describe different variants of Naive Bayes, such as Gaussian, Multinomial, and Bernoulli Naive Bayes, and explain when to use each

Bayes Theorem is a mathematical theorem used for predicting a future outcome depending on a collected piece of evidence. More specifically, this theorem calculates "conditional probabilities", which depicts the probability of an outcome based on a prior condition/ event. The formula for the theorem is: 
$\[P(A|B) = (P(B|A) (P(A))) / P(B) \]
In this formula, to predict the probability of event A given event B, we multiply the probability of event B when event A occurs by the probability of event A. We then divide the product by the total probability of event B.
With that in mind, Naive Bayes is a...
There are many variants under the umbrella of Naive Bayes. Included in this bunch are Complement Naive Bayes, Out- of- core Naive Bayes model- fitting, Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes. When speaking of Gaussian Naive Bayes... Whereas, Bernoulli Naive Bayes... Likewise, Multinomial Naive Bayes... 
(Week 7 lecture)


## Data Prep

To prepare for Naive Bayes classification, I split my dataset into training and testing datasets. I do this by .. Once partitioned, there should be a training set-- which consists of the training set as well as a validation set, to do a minor evaluation on what has been trained-- and a test set. The purpose of the partitions of different sets is so I can inform my model of what categories to look for when actually classifying my collected datasets.


## Feature Selection

You need to use either R or Python to code Naive Bayes (NB) as a classification model for your data, we will use NB as a wrapper for feature selection. While not required, if you want to take it further, you can try this in both R and Python.

Objective: The primary objective of the Feature Selection component in this project is to identify and choose the most relevant and informative features (variables or attributes) from the dataset, for the given task. Effective feature selection can improve the model’s performance, reduce overfitting, and enhance the interpretability of the results.

Instructions: Generalize and apply the code in the lab assignment and lab-demonstration called “Feature selection with text data” to the text and record data you have collected for your project. This code demonstrates feature selection for a text classification task, so map the task onto your projects dataset.


## Results- Record Data

Using your optimal feature set from the previous section, fit a final “optimal” NB model for your Record data.
Report and comment on the findings. It is required that you create code, appropriate visualizations, result summaries, confusion matrices, etc
Describe how the trained model is tested on the testing dataset.
Discuss the evaluation metrics used to assess the performance of the Naive Bayes classifier (e.g., accuracy, precision, recall, F1-score).
Discuss the concepts of overfitting and under-fitting and whether your model is doing it.
Discuss the model’s performance in terms of accuracy and other relevant metrics.
Describe how the project findings will be documented and reported, including the format of reports or presentations.
e.g. what is the output that you generate. What does the output mean? What does it tell you about your data? Does your model do a good job of predicting your test data? Include and discuss relevant visualizations, results, the confusion matrices, etc .
Create and include a minimum of three visualizations for each case (text and record classification).
Write a conclusion paragraph interpreting the results. Note, this is not the same as a write-up of technical methodological details.