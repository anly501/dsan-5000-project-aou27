[
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression",
    "section": "",
    "text": "Code\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nfrom sklearn import tree\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\nhappydf = pd.read_csv(\"../websitedata/happy_py.csv\")\n\nappdf = pd.read_csv(\"../websitedata/apple_py.csv\")\n\n\n\n\nCode\nhappydf = happydf.sort_values(by=\"Score\", ascending=True)\nhappydf = happydf.drop(columns={\"Unnamed: 0\", \"Code\"}, axis=1)\nhappy20 = pd.concat([happydf.head(20), happydf.tail(20)])\nhappy20 = happy20.rename(columns={\"Entity\": \"CountryHappy\"})\n\nhappy20 = happy20.sort_values(by=[\"CountryHappy\"])\nhappy20 = happy20.reset_index(drop=True)\n#print(happy20)\nprint(happy20.head)\n\n\n&lt;bound method NDFrame.head of                     CountryHappy   Score         GDP\n0                    Afghanistan  1.8590       363.7\n1                      Australia  7.0946   64,491.40\n2                        Austria  7.0973   52,131.40\n3                     Bangladesh  4.2821    2,688.30\n4                        Belgium  6.8591   49,582.80\n5                       Botswana  3.4353    7,737.70\n6                         Canada  6.9607   54,966.50\n7                        Comoros  3.5452    1,484.90\n8                        Czechia  6.8452   27,638.40\n9   Democratic Republic of Congo  3.2072       586.5\n10                       Denmark  7.5864   66,983.10\n11                         Egypt  4.1705    4,295.40\n12                      Ethiopia  4.0906    1,027.60\n13                       Finland  7.8042   50,536.60\n14                        Gambia  4.2794         840\n15                       Germany  6.8918   48,432.50\n16                       Iceland  7.5296   72,903.00\n17                         India  4.0358    2,388.60\n18                       Ireland  6.9108  104,038.90\n19                        Israel  7.4729   54,659.80\n20                        Jordan  4.1198    4,204.50\n21                       Lebanon  2.3922    4,136.10\n22                       Liberia  4.0423       754.5\n23                     Lithuania  6.7630   24,826.80\n24                    Luxembourg  7.2279  126,426.10\n25                    Madagascar  4.0191         505\n26                        Malawi  3.4952       645.2\n27                          Mali  4.1978       833.3\n28                   Netherlands  7.4030   55,985.40\n29                   New Zealand  7.1229   48,249.30\n30                        Norway  7.3155  106,148.80\n31                  Sierra Leone  3.1376       461.4\n32                        Sweden  7.3952   55,873.20\n33                   Switzerland  7.2401   92,101.50\n34                      Tanzania  3.6938    1,192.40\n35                          Togo  4.1374       918.4\n36                United Kingdom  6.7956   45,850.40\n37                 United States  6.8937   76,398.60\n38                        Zambia  3.9822    1,487.90\n39                      Zimbabwe  3.2035    1,267.00&gt;\n\n\n\n\nCode\ncountry_mapping = {\n    \"US\": \"United States\",'UK': 'United Kingdom','JP': 'Japan','DE': 'Germany','AU': 'Australia','CA': 'Canada',\n    'FR': 'France','IT': 'Italy','KR': 'South Korea','MX': 'Mexico','TH': 'Thailand','BE': 'Belgium','BR': 'Brazil',\n    'CH': 'Switzerland','CN': 'China','CO': 'Colombia','ES': 'Spain','HK': 'Hong Kong','ID': 'Indonesia',\n    'IE': 'Ireland','IN': 'India','NL': 'Netherlands','NZ': 'New Zealand','TR': 'Turkey','TW': 'Taiwan','ZA': 'South Africa',\n    'AE': 'United Arab Emirates','AR': 'Argentina','AT': 'Austria','CL': 'Chile','CZ': 'Czech Republic','DK': 'Denmark',\n    'EE': 'Estonia','EG': 'Egypt','FI': 'Finland','GR': 'Greece','HU': 'Hungary','IL': 'Israel','KE': 'Kenya','KZ': 'Kazakhstan',\n    'LB': 'Lebanon','LT': 'Lithuania','LU': 'Luxembourg','MY': 'Malaysia','NG': 'Nigeria','NO': 'Norway','PE': 'Peru',\n    'PH': 'Philippines','PL': 'Poland','PT': 'Portugal','RO': 'Romania','SA': 'Saudi Arabia','SE': 'Sweden',\n    'SI': 'Slovenia','SG': 'Singapore','SK': 'Slovakia','UA': 'Ukraine','VN': 'Vietnam'}\nappdf[\"Country\"] = appdf[\"Country\"].map(country_mapping)\n\nappdf = appdf.loc[(appdf[\"Peak\"] == 1)]\nappdf = appdf.drop_duplicates(subset=\"Country\", keep=\"last\") #did last bc first was all Doja Cat and want to see some variation\nappdf = appdf.drop(columns={\"Unnamed: 0\"})\n\nappdf = appdf.head(40)\n\nappdf = appdf.sort_values(by=[\"Country\"])\nappdf = appdf.reset_index(drop=True)\n\nprint(appdf.head)\n\n\n&lt;bound method NDFrame.head of     Chart Position                    Artist and Title  Position Duration  \\\n0              198  Elton John & Dua Lipa - Cold Heart                787   \n1              198  Elton John & Dua Lipa - Cold Heart                787   \n2               84             Travis Scott - MELTDOWN                 76   \n3              126                Ed Sheeran - Perfect               1982   \n4              198  Elton John & Dua Lipa - Cold Heart                787   \n5              126                Ed Sheeran - Perfect               1982   \n6                7         Taylor Swift - Cruel Summer                245   \n7                7         Taylor Swift - Cruel Summer                245   \n8               42            Harry Styles - As It Was                559   \n9              126                Ed Sheeran - Perfect               1982   \n10             198  Elton John & Dua Lipa - Cold Heart                787   \n11              84             Travis Scott - MELTDOWN                 76   \n12              49        The Weeknd - Blinding Lights               1354   \n13             126                Ed Sheeran - Perfect               1982   \n14              32               Miley Cyrus - Flowers                272   \n15              84             Travis Scott - MELTDOWN                 76   \n16              18          Dua Lipa - Dance The Night                139   \n17             126                Ed Sheeran - Perfect               1982   \n18              32               Miley Cyrus - Flowers                272   \n19             126                Ed Sheeran - Perfect               1982   \n20             126                Ed Sheeran - Perfect               1982   \n21             198  Elton John & Dua Lipa - Cold Heart                787   \n22               6            Olivia Rodrigo - vampire                104   \n23              84             Travis Scott - MELTDOWN                 76   \n24               1       Doja Cat - Paint The Town Red                 68   \n25               1       Doja Cat - Paint The Town Red                 68   \n26              84             Travis Scott - MELTDOWN                 76   \n27              42            Harry Styles - As It Was                559   \n28             198  Elton John & Dua Lipa - Cold Heart                787   \n29             198  Elton John & Dua Lipa - Cold Heart                787   \n30             198  Elton John & Dua Lipa - Cold Heart                787   \n31              70                     SZA - Kill Bill                307   \n32              42            Harry Styles - As It Was                559   \n33             198  Elton John & Dua Lipa - Cold Heart                787   \n34               7         Taylor Swift - Cruel Summer                245   \n35             126                Ed Sheeran - Perfect               1982   \n36               1       Doja Cat - Paint The Town Red                 68   \n37             126                Ed Sheeran - Perfect               1982   \n38             126                Ed Sheeran - Perfect               1982   \n39              84             Travis Scott - MELTDOWN                 76   \n\n    Peak  (x?)  Points               Country  Country Rank  \n0      1    41    1056             Argentina          93.0  \n1      1    41    1056             Australia          43.0  \n2      1     3    1860               Austria         153.0  \n3      1     2    1446               Belgium         126.0  \n4      1    41    1056                Brazil          68.0  \n5      1     2    1446                Canada         150.0  \n6      1    13   10731                 Chile         123.0  \n7      1    13   10731                 China          71.0  \n8      1   177    4017              Colombia         186.0  \n9      1     2    1446        Czech Republic          96.0  \n10     1    41    1056               Denmark         124.0  \n11     1     3    1860                 Egypt          47.0  \n12     1   114    3624               Estonia          29.0  \n13     1     2    1446               Finland         158.0  \n14     1   168    5145                France         124.0  \n15     1     3    1860               Germany         170.0  \n16     1    19    7035                Greece          92.0  \n17     1     2    1446             Hong Kong         182.0  \n18     1   168    5145               Hungary          74.0  \n19     1     2    1446                 India         103.0  \n20     1     2    1446             Indonesia         138.0  \n21     1    41    1056               Ireland         151.0  \n22     1    22   10868                Israel          21.0  \n23     1     3    1860                 Italy         144.0  \n24     1    45   16920                 Japan         165.0  \n25     1    45   16920            Kazakhstan           2.0  \n26     1     3    1860                 Kenya          59.0  \n27     1   177    4017                Mexico         171.0  \n28     1    41    1056           Netherlands          99.0  \n29     1    41    1056           New Zealand          49.0  \n30     1    41    1056          South Africa         177.0  \n31     1     3    2487           South Korea          98.0  \n32     1   177    4017                 Spain         164.0  \n33     1    41    1056           Switzerland         122.0  \n34     1    13   10731                Taiwan          20.0  \n35     1     2    1446              Thailand         131.0  \n36     1    45   16920                Turkey          21.0  \n37     1     2    1446  United Arab Emirates          91.0  \n38     1     2    1446        United Kingdom         136.0  \n39     1     3    1860         United States          52.0  &gt;\n\n\n\n\nCode\n# Concatenate then vectorize countries\nprint(appdf.shape)\nhappymusicdf = pd.concat([happy20, appdf],axis=1)\nprint(happymusicdf.shape)\nprint(happymusicdf.keys())\nprint(happymusicdf.head)\n\n\n(40, 8)\n(40, 11)\nIndex(['CountryHappy', 'Score', 'GDP', 'Chart Position', 'Artist and Title',\n       'Position Duration', 'Peak', '(x?)', 'Points', 'Country',\n       'Country Rank'],\n      dtype='object')\n&lt;bound method NDFrame.head of                     CountryHappy   Score         GDP  Chart Position  \\\n0                    Afghanistan  1.8590       363.7             198   \n1                      Australia  7.0946   64,491.40             198   \n2                        Austria  7.0973   52,131.40              84   \n3                     Bangladesh  4.2821    2,688.30             126   \n4                        Belgium  6.8591   49,582.80             198   \n5                       Botswana  3.4353    7,737.70             126   \n6                         Canada  6.9607   54,966.50               7   \n7                        Comoros  3.5452    1,484.90               7   \n8                        Czechia  6.8452   27,638.40              42   \n9   Democratic Republic of Congo  3.2072       586.5             126   \n10                       Denmark  7.5864   66,983.10             198   \n11                         Egypt  4.1705    4,295.40              84   \n12                      Ethiopia  4.0906    1,027.60              49   \n13                       Finland  7.8042   50,536.60             126   \n14                        Gambia  4.2794         840              32   \n15                       Germany  6.8918   48,432.50              84   \n16                       Iceland  7.5296   72,903.00              18   \n17                         India  4.0358    2,388.60             126   \n18                       Ireland  6.9108  104,038.90              32   \n19                        Israel  7.4729   54,659.80             126   \n20                        Jordan  4.1198    4,204.50             126   \n21                       Lebanon  2.3922    4,136.10             198   \n22                       Liberia  4.0423       754.5               6   \n23                     Lithuania  6.7630   24,826.80              84   \n24                    Luxembourg  7.2279  126,426.10               1   \n25                    Madagascar  4.0191         505               1   \n26                        Malawi  3.4952       645.2              84   \n27                          Mali  4.1978       833.3              42   \n28                   Netherlands  7.4030   55,985.40             198   \n29                   New Zealand  7.1229   48,249.30             198   \n30                        Norway  7.3155  106,148.80             198   \n31                  Sierra Leone  3.1376       461.4              70   \n32                        Sweden  7.3952   55,873.20              42   \n33                   Switzerland  7.2401   92,101.50             198   \n34                      Tanzania  3.6938    1,192.40               7   \n35                          Togo  4.1374       918.4             126   \n36                United Kingdom  6.7956   45,850.40               1   \n37                 United States  6.8937   76,398.60             126   \n38                        Zambia  3.9822    1,487.90             126   \n39                      Zimbabwe  3.2035    1,267.00              84   \n\n                      Artist and Title  Position Duration  Peak  (x?)  Points  \\\n0   Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n1   Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n2              Travis Scott - MELTDOWN                 76     1     3    1860   \n3                 Ed Sheeran - Perfect               1982     1     2    1446   \n4   Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n5                 Ed Sheeran - Perfect               1982     1     2    1446   \n6          Taylor Swift - Cruel Summer                245     1    13   10731   \n7          Taylor Swift - Cruel Summer                245     1    13   10731   \n8             Harry Styles - As It Was                559     1   177    4017   \n9                 Ed Sheeran - Perfect               1982     1     2    1446   \n10  Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n11             Travis Scott - MELTDOWN                 76     1     3    1860   \n12        The Weeknd - Blinding Lights               1354     1   114    3624   \n13                Ed Sheeran - Perfect               1982     1     2    1446   \n14               Miley Cyrus - Flowers                272     1   168    5145   \n15             Travis Scott - MELTDOWN                 76     1     3    1860   \n16          Dua Lipa - Dance The Night                139     1    19    7035   \n17                Ed Sheeran - Perfect               1982     1     2    1446   \n18               Miley Cyrus - Flowers                272     1   168    5145   \n19                Ed Sheeran - Perfect               1982     1     2    1446   \n20                Ed Sheeran - Perfect               1982     1     2    1446   \n21  Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n22            Olivia Rodrigo - vampire                104     1    22   10868   \n23             Travis Scott - MELTDOWN                 76     1     3    1860   \n24       Doja Cat - Paint The Town Red                 68     1    45   16920   \n25       Doja Cat - Paint The Town Red                 68     1    45   16920   \n26             Travis Scott - MELTDOWN                 76     1     3    1860   \n27            Harry Styles - As It Was                559     1   177    4017   \n28  Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n29  Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n30  Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n31                     SZA - Kill Bill                307     1     3    2487   \n32            Harry Styles - As It Was                559     1   177    4017   \n33  Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n34         Taylor Swift - Cruel Summer                245     1    13   10731   \n35                Ed Sheeran - Perfect               1982     1     2    1446   \n36       Doja Cat - Paint The Town Red                 68     1    45   16920   \n37                Ed Sheeran - Perfect               1982     1     2    1446   \n38                Ed Sheeran - Perfect               1982     1     2    1446   \n39             Travis Scott - MELTDOWN                 76     1     3    1860   \n\n                 Country  Country Rank  \n0              Argentina          93.0  \n1              Australia          43.0  \n2                Austria         153.0  \n3                Belgium         126.0  \n4                 Brazil          68.0  \n5                 Canada         150.0  \n6                  Chile         123.0  \n7                  China          71.0  \n8               Colombia         186.0  \n9         Czech Republic          96.0  \n10               Denmark         124.0  \n11                 Egypt          47.0  \n12               Estonia          29.0  \n13               Finland         158.0  \n14                France         124.0  \n15               Germany         170.0  \n16                Greece          92.0  \n17             Hong Kong         182.0  \n18               Hungary          74.0  \n19                 India         103.0  \n20             Indonesia         138.0  \n21               Ireland         151.0  \n22                Israel          21.0  \n23                 Italy         144.0  \n24                 Japan         165.0  \n25            Kazakhstan           2.0  \n26                 Kenya          59.0  \n27                Mexico         171.0  \n28           Netherlands          99.0  \n29           New Zealand          49.0  \n30          South Africa         177.0  \n31           South Korea          98.0  \n32                 Spain         164.0  \n33           Switzerland         122.0  \n34                Taiwan          20.0  \n35              Thailand         131.0  \n36                Turkey          21.0  \n37  United Arab Emirates          91.0  \n38        United Kingdom         136.0  \n39         United States          52.0  &gt;\n\n\n\n\nCode\nhappysongsdf = happymusicdf[happymusicdf[\"CountryHappy\"] == happymusicdf[\"Country\"]]\nhappysongsdf[\"GDP\"] = pd.to_numeric(happysongsdf[\"GDP\"].str.replace(',', ''), errors='coerce')\nprint(happysongsdf.shape)\nprint(happysongsdf.head)\n\n#happysongsdf.to_csv(\"../websitedata/happyCountryMatch.csv\")\n\n\n(9, 11)\n&lt;bound method NDFrame.head of    CountryHappy   Score      GDP  Chart Position  \\\n1     Australia  7.0946  64491.4             198   \n2       Austria  7.0973  52131.4              84   \n10      Denmark  7.5864  66983.1             198   \n11        Egypt  4.1705   4295.4              84   \n13      Finland  7.8042  50536.6             126   \n15      Germany  6.8918  48432.5              84   \n28  Netherlands  7.4030  55985.4             198   \n29  New Zealand  7.1229  48249.3             198   \n33  Switzerland  7.2401  92101.5             198   \n\n                      Artist and Title  Position Duration  Peak  (x?)  Points  \\\n1   Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n2              Travis Scott - MELTDOWN                 76     1     3    1860   \n10  Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n11             Travis Scott - MELTDOWN                 76     1     3    1860   \n13                Ed Sheeran - Perfect               1982     1     2    1446   \n15             Travis Scott - MELTDOWN                 76     1     3    1860   \n28  Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n29  Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n33  Elton John & Dua Lipa - Cold Heart                787     1    41    1056   \n\n        Country  Country Rank  \n1     Australia          43.0  \n2       Austria         153.0  \n10      Denmark         124.0  \n11        Egypt          47.0  \n13      Finland         158.0  \n15      Germany         170.0  \n28  Netherlands          99.0  \n29  New Zealand          49.0  \n33  Switzerland         122.0  &gt;\n\n\nC:\\Users\\Amand\\AppData\\Local\\Temp\\ipykernel_19352\\3590638393.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  happysongsdf[\"GDP\"] = pd.to_numeric(happysongsdf[\"GDP\"].str.replace(',', ''), errors='coerce')\n\n\n\n\nCode\nhappysongs = happysongsdf.drop(columns={\"Country\", \"CountryHappy\", \"Score\", \"Artist and Title\"})\n#happysongsdf = happysongsdf.rename(columns={\"CountryHappy\": \"Country\"})\nprint(happysongs)\n\n\n        GDP  Chart Position  Position Duration  Peak  (x?)  Points  \\\n1   64491.4             198                787     1    41    1056   \n2   52131.4              84                 76     1     3    1860   \n10  66983.1             198                787     1    41    1056   \n11   4295.4              84                 76     1     3    1860   \n13  50536.6             126               1982     1     2    1446   \n15  48432.5              84                 76     1     3    1860   \n28  55985.4             198                787     1    41    1056   \n29  48249.3             198                787     1    41    1056   \n33  92101.5             198                787     1    41    1056   \n\n    Country Rank  \n1           43.0  \n2          153.0  \n10         124.0  \n11          47.0  \n13         158.0  \n15         170.0  \n28          99.0  \n29          49.0  \n33         122.0  \n\n\n\n\nCode\n# SCORE AND CHART POSITION\nX = happysongs\ny = happysongsdf[\"Score\"]\ny = y.to_numpy()\n\nprint(X.shape)\nprint(y.shape)\n\n\n(9, 7)\n(9,)\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\n\nx_train.shape        : (7, 7)\ny_train.shape        : (7,)\nX_test.shape     : (2, 7)\ny_test.shape     : (2,)\n\n\n\n\nCode\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\n\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(1,40):\n    # INITIALIZE MODEL \n    model = DecisionTreeRegressor(max_depth=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i==1 or i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\n\nhyperparam = 1\n train error: 0.19910476190476142\n test error: 0.23175000000000034\nhyperparam = 10\n train error: 0.0\n test error: 0.28869999999999996\nhyperparam = 20\n train error: 0.0\n test error: 0.18730000000000002\nhyperparam = 30\n train error: 0.0\n test error: 0.18730000000000002\n\n\n\n\nCode\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Depth of tree (max depth)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\ni=1\nprint(hyper_param[i],train_error[i],test_error[i])\n\n\n2 0.10067999999999991 0.23175000000000034\n\n\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint(\"TRAINING SHAPES:\",x_train.shape,y_train.shape)\nprint(\"TEST SHAPES:\",x_test.shape,y_test.shape)\n\n\nTRAINING SHAPES: (6, 7) (6,)\nTEST SHAPES: (3, 7) (3,)\n\n\n\n\nCode\n# INITIALIZE MODEL \nmodel = DecisionTreeRegressor(max_depth=1)\nmodel.fit(x_train,y_train)                     # TRAIN MODEL \n\n\n# OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\nerr1=mean_absolute_error(y_train, yp_train) \nerr2=mean_absolute_error(y_test, yp_test) \n    \nprint(\" train error:\",err1)\nprint(\" test error:\" ,err2)\n\n\n train error: 0.21123999999999996\n test error: 0.217646666666667\n\n\nMy dataset was too small for a supervised learning project to yield an accurate analysis. However, my methodology was to run a regression model, where I combine the datasets for self- reported life satisfaction and apple music rankings and use the life satisfaction as the target variable. The decision tree produced 2 leaves, one with a value of 4.17 and another with a value of 7.287.\nBelow, I produced a decision tree with some information on the success of the algorithm and a linear regression visualization. As the closer the predicted y and true y is to the line, this visualization shows the fit was not perfect, but luckily the data did not suffer from overfitting.\n\n\nCode\nfrom sklearn import tree\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\nplot_tree(model)\n\n\n\n\n\n\n\nCode\n# LINEAR REGRESSION \nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression().fit(X, y)\n\n# OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\nplt.plot(y_train,yp_train ,\"o\", color='k')\nplt.plot(y_test,yp_test ,\"o\", color='b')\nplt.plot(y_train,y_train,\"-\", color='r')\n\nplt.xlabel(\"y_data\")\nplt.ylabel(\"y_pred (blue=test)(black=Train)\")\n\n    \nerr1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\nerr2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\nprint(\" train error:\",err1)\nprint(\" test error:\" ,err2)\n\n\n train error: 5.476143871360148\n test error: 5.467027259259829"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Mood, Music, and Media",
    "section": "",
    "text": "As the prevalence of daily technological usage grows, streaming platforms continue to permeate the infrastructure of societal pastimes. Music streaming platforms such as Apple Music and Spotify, as well as streaming services such as Hulu and Netflix, are becoming increasingly popular due to providing a plethora of entertainment at one’s fingertips. Due to this distinctive growth in an array of highly- accessible media, a desire to cater to individual consumers– and a desire by consumers to be presented with more specialized content– is also increasing. Inspired by works already implemented, my project aims to examine if the self- reported happiness level of a person in the past- year will have a significant impact on their media consumption preferences today."
  },
  {
    "objectID": "introduction.html#project",
    "href": "introduction.html#project",
    "title": "Mood, Music, and Media",
    "section": "Project",
    "text": "Project\nAs an attempt to build on these efforts, my project utilizes algorithms such as Naive Bayes Classifier and Decision Trees to examine what conclusions can be drawn from the level of international subjective satisfaction and worldwide media preferences. In my project, I plan to address questions such as:\n\nIs there a strong preference for negative content worldwide?\nIs life satisfaction significantly tied to the Gross Domestic Product per capita of where you live?\nIs there a strong preference for positive or neutral content worldwide?\nWhat songs are more likely to be preferred by which countries?\nIs there a correlation between Country Rank on Apple Music and Peak and the self- reported life satisfaction data influence the highest position?\nDoes more energy mean more positivity in songs?\nWould danceability be the biggest factor in how positive or negative a song is?\nIs favored modern media more negative or positive?\nIs favored modern music more negative or positive?\nWhich country is most likely to choose a happy song over a sad song?"
  },
  {
    "objectID": "dimensionalityreduction.html",
    "href": "dimensionalityreduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "For dimensionality reduction, I aim to look at the most important features in my Apple Music dataset. I will be using Principal Component Analysis and T- distributed Stochastic Neighbor Embedding to achieve this. The libraries I will be using include request, json, re, pandas, numpy, seaborn, and matplotlib,pyplot. For the techniques specifically, I will be using decomposition from Sci- Kit Learn for the Principal Component Analysis and manifold from Sci- Kit Learn for the T- distributed Stochastic Neighbor Embedding.\n\nPCA\n\n\nCode\nimport requests\nimport json\nimport re\n#import pycountry\nimport pandas as pd\nimport numpy as np\n\nappledf = pd.read_csv(\"../websitedata/apple_py.csv\")\n\nnewAppledf = pd.read_csv(\"../websitedata/newApple_py.csv\")\n\n\n\n\nCode\ny = appledf[\"Peak\"]\ny=np.array(y)\n\nx= newAppledf.drop(columns={\"Peak\", \"Unnamed: 0\"})\nprint(x.head)\n\n\n&lt;bound method NDFrame.head of      Chart Position  Position Duration  (x?)  Points  Country Rank\n0                 1                 68    45   16920            29\n1                 2                 27     9   14619            44\n2                 3                  6     1   13364             1\n3                 4                  6     2   12624             2\n4                 5                  6     2   12359             3\n..              ...                ...   ...     ...           ...\n831               7                245    13   10731            28\n832              30                 90    10    5425           166\n833              33                356    50    5108           146\n834              62                412    13    2739           116\n835              70                307     3    2487           104\n\n[836 rows x 5 columns]&gt;\n\n\n\n\nCode\n# LOAD DATA\nX= x\nY= y\n\n#NORMALIZE AND RESHAPE\nX=X/np.max(X) #NORMALIZE\n# X=X.reshape(60000,28*28); #print(X[0])\ncov_X = X.cov()\nprint(cov_X)\n\n\n                   Chart Position  Position Duration          (x?)    Points  \\\nChart Position       8.667260e-06           0.000040 -3.608966e-07 -0.000697   \nPosition Duration    3.996314e-05           0.000639  1.775432e-05 -0.004122   \n(x?)                -3.608966e-07           0.000018  7.882437e-06 -0.000151   \nPoints              -6.968281e-04          -0.004122 -1.509267e-04  0.090143   \nCountry Rank         4.868267e-06           0.000026  1.687945e-07 -0.000486   \n\n                   Country Rank  \nChart Position     4.868267e-06  \nPosition Duration  2.598144e-05  \n(x?)               1.687945e-07  \nPoints            -4.861844e-04  \nCountry Rank       9.751437e-06  \n\n\n\n\nCode\n# APPLYING PCA TO WHOLE DATASET\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#COMPUTE PCA\nfrom sklearn.decomposition import PCA\nn_components = X.shape[1]\npca = PCA(n_components=n_components)\n# pca.fit(X)\nX1=pca.fit_transform(X)\n\n#2D PLOT; for Chart Position and Position Duration\nax = plt.scatter(X1[:,0], X1[:,1], c=Y, cmap='tab10')\nplt.title(\"PCA Plot: Chart Position vs Position Duration\")\nplt.xlabel(\"Chart Position\")\nplt.ylabel(\"Position Duration\")\nplt.colorbar(ax, label=\"Dimensions\")\nplt.legend()  \nplt.grid(True)\nplt.show()\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\nCode\n#COMPUTE PRINCIPAL COMPONENTS use min instead of mle due to it being a small dataset\nn_components = min(X.shape[0], X.shape[1])\n\nprint(n_components)\n\n\n5\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#COMPUTE PCA\nfrom sklearn.decomposition import PCA\nn_components = min(X.shape[0], X.shape[1])\npca = PCA(n_components=n_components)\n# pca.fit(X)\nX1=pca.fit_transform(X)\n\n#2D PLOT; for Chart Position and Country Rank\n# 2D PLOT; for (x?) and Points\n#2D PLOT; for Chart Position and Position Duration\n\nax = plt.scatter(X1[:,0], X1[:,1], c=Y, cmap='tab10')\nplt.title(\"PCA Plot: Chart Position vs Position Duration\")\nplt.xlabel(\"Chart Position\")\nplt.ylabel(\"Country Rank\")\nplt.colorbar(ax, label=\"Dimensions\")\nplt.legend()  \nplt.grid(True)\nplt.show()\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\nt-SNE\n\n\nCode\nimport numpy as np\nfrom sklearn.manifold import TSNE\n\n# DO DIMENSIONALITY REDUCTION\nX_embedded = TSNE(n_components=3, learning_rate='auto',init='random', perplexity=5).fit_transform(X)\n\n# EXPLORE RESULTS\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\n# PLOT \nplt.scatter(X_embedded[:,0],X_embedded[:,1], alpha=0.5)\nplt.title(\"PCA Plot: Chart Position vs Position Duration\")\nplt.xlabel(\"Chart Position\")\nplt.ylabel(\"Country Rank\")\nplt.legend()  \nplt.grid(True)\nplt.show()\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\nRESULTS\nshape :  (836, 3)\nFirst few points : \n [[ 6.175029   8.508618  -7.4983315]\n [ 8.387154   3.5311518 13.786998 ]]\n\n\n\n\n\n\n\nCode\n# DIFFERENT PERPLEXITY\nX_embedded = TSNE(n_components=3, learning_rate='auto',init='random', perplexity=10).fit_transform(X)\n\n# EXPLORE RESULTS\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\n# PLOT \nplt.scatter(X_embedded[:,0],X_embedded[:,1], alpha=0.5)\nplt.title(\"PCA Plot: Chart Position vs Position Duration\")\nplt.xlabel(\"Chart Position\")\nplt.ylabel(\"Country Rank\")\nplt.legend()  \nplt.grid(True)\nplt.show()\nplt.show()\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\nRESULTS\nshape :  (836, 3)\nFirst few points : \n [[ 3.7008498 -4.7721825 14.357299 ]\n [14.687176  -8.408725   6.7310157]]\n\n\n\n\n\n\n\nCode\n# DIFFERENT PERPLEXITY\nX_embedded = TSNE(n_components=3, learning_rate='auto',init='random', perplexity=15).fit_transform(X)\n\n# EXPLORE RESULTS\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\n# PLOT \nplt.scatter(X_embedded[:,0],X_embedded[:,1], alpha=0.5)\nplt.title(\"PCA Plot: Chart Position vs Position Duration\")\nplt.xlabel(\"Chart Position\")\nplt.ylabel(\"Country Rank\")\nplt.legend()  \nplt.grid(True)\nplt.show()\nplt.show()\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\nRESULTS\nshape :  (836, 3)\nFirst few points : \n [[-10.657185   -3.6224663  -1.5625678]\n [  1.7244056 -20.046017   -4.3512034]]\n\n\n\n\n\nWhen observing the Principal Component Analysis, it seems there is more variance as the songs have lower (numbers increase for rankings) Country Rank and Chart Position. For t- SNE, it appears the clusters became noticeably distinct as the perplexity reached 15. This might indicate the Apple Music dataset contains pretty distinct clusters in general."
  },
  {
    "objectID": "datagathering.html",
    "href": "datagathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "The data I gathered to examine how the sentiments of current music and media coincide with the general happiness levels of different countries consists of text data, qualitative record data, and quantitative record data. The data comes from five different main sources, two of which are directly from website API (Application Programming Interface) sources. As an overview of the data sources:\n\nApple Music International Rankings: Displays the Apple Music rankings for the top international songs in October 2023.\nSpotify: Contains variables that give a summary of different aspects of an artist’s various songs and albums over the years\nTop Netflix TV Shows in 2022: Consists of the top television shows worldwide on Netflix’s streaming platform.\nGDP vs. Happiness: This data explores the various levels of happiness and the Gross Domestic Product (GDP) per capita spanning different years for each country.\nWikipedia: Contains text data from specific WIkipedia topic pages.\n\n\nApple Music\nOne of my goals for this project was to get a large dataset representing the current top rated songs in various countries. After searching a bit, I was able to discover Kworb’s website filled with real- time data on how Apple Music’s international listeners are ranking songs. Below is a picture of the raw data, which shows the artist, their song, their song’s ranking in different countries, the duration of their ranking position, and the amounts of points accumulated that verify their ranking position.\n\n\n\nApple Music International Rankings Raw Dataset\n\n\n\n\nSpotify API\nMy Spotify content required authorization to access. At the very bottom of both my .R and .rmd files in my “Rcode” folder under my “websitebackupcode” folder, you will find how I set up my data collection for my Spotify dataset. I first submitted a registering application on Spotify’s API website. Once receiving my Client ID key and my Client Secret key, I used the keys to request a Spotify access token. This gave me access to the artist and corresponding song information available in Spotify’s Web API.\nI wanted to gain insights into the first five artists at the top of the ranking for international songs from Apple Music. So, I requested Spotify information on Doja Cat, Tate McRae, Drake, Olivia Rodrigo, and Taylor Swift. The request yielded information on each song’s name, artist, valence, danceability, energy, loudness, speechiness, acousticness, liveness, tempo, album, and album release year. Which can be found here. As a brief overview:\n\nValence: A metric of musical positivity.\nDanceability\nEnergy: A metric of , loudness, and noise in a song.\nLoudness: A metric for the loudness; measured in decibels.\nSpeechiness: A metric for the presence of speech- adjacent aspects in a recording.\nAcousticness: A metric denoting how acoustic the song was.\nLiveness: A metric concerning the level of an audience presence in the recording.\nTempo: A metric of the estimated tempo of each song.\n\nRight below this, is a snapshot of me using my Client ID key and Client Secret key to request API data access.\n\n\n\nSpotify API Requesting and Formation\n\n\n\n\nNetflix\nOne of my other goals for this project was to find data that represents the most watched television shows on a specific streaming platform in the past year. I was able to come across Statista’s arsenal. Statista provided data on a survey done by Flix Patrol, which captured the most watched Netflix television shows internationally as of December 2022. The dataset was modest; only consisting of the television shows in order of ranking and their corresponding points that justify their ranking. As stated on Statista, and clarified here, the points system is designed to be, “always based on the popularity ranking coming directly from the streaming platforms” ( (n.d.)). The total scores are an accumulation of points that fit this breakdown:\n\n10 points for the #1 television show\n9 points for the #2 television show . . .\n1 point for the #10 television show\n\nBelow this is a picture of the original histogram visual of data provided by Statista, (Patrol and 3 (2023)).\n\n\n\nStatista’s Netflix Raw Data Visualization\n\n\n\n\nGDP vs Self- Reported Life Satisfaction\nThe anchor connecting my music and media streaming platforms’ data is a dataset that would reflect some sort of emotional status of many different countries. Luckily, I discovered Our World in Data, which had a plethora of data reflecting interesting societal topics– including the relationship of subjective personal satisfaction and the origin country’s Gross Domestic Product (GDP) per capita. Our World in Data allowed me to easily download the dataset directly from their website. The dataset contains 3 variables:\n\nThe population for each country at different years up until the end of 2022\nThe GDP per capita in international dollars (a normalized currency they used for comparison); this dataset borrowed statistics from 2017\nThe country average of life satisfaction. The scale used is the Cantril Ladder, which ranges from 0 to 10 (“0” being the lowest rating of life satisfaction and “10” being the highest rating of life satisfaction).\n\nBelow this is a visualization of the raw data provided by Our World in Data, which showcases the relationship of the countries’ GDP per capita and self- reported life satisfaction levels( (2022)).\n\n\n\nOur World in Data’s Raw Data Visualization\n\n\n\n\nWikipedia API\nMy one other dataset source generated from an API call is the datasets of my Wikipedia API. The Python language has a wikipedia library that makes it easy for programmers to access any available wikipedia page content. For setting up my datasets from the wikipedia library, I imported the library to my workspace. I then requested the wikipedia page for all the television shows in the top five of my previously mentioned “Top TV shows on Netflix worldwide 2022” data. I was interested in using the plots for my analysis, so I only took the “Premise” portions of all of my wikipedia requests.\nA description and example of the abilities of the wikipedia library for the Python language can be found here. Additionally, My entire data collection process for the Wikipedia API calls can be found in my “pythonApiBackup” folder inside my “websitebackupfolder” in my Github repository.\nBelow, is a screenshot of the beginning of my wikipedia API call for the highest ranked show from my “Top TV shows on Netflix worldwide 2022” dataset.\n\n\n\nWikipedia API Call\n\n\n\n\n\n\n\nReferences\n\n2022. Our World in Data. https://ourworldindata.org/grapher/gdp-vs-happiness.\n\n\n———. n.d. FlixPatrol. https://flixpatrol.com/about/how-it-works/.\n\n\nPatrol, Flix, and Nov 3. 2023. “Netflix: Top TV Shows 2022.” Statista. 2020- 2023 Flix Patrol. https://www.statista.com/statistics/1361250/netflix-top-tv-shows-worldwide/."
  },
  {
    "objectID": "DataCleaning.html",
    "href": "DataCleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "All of my detailed data cleaning practices are linked with steps in R and Python. These documents can be found inside the “websitebackupcode” folder on my linked Github repository for my project. This link can be found under the “Data” tab of the website.\n\nData Preparation for Analysis\nIn order to prepare my data for analysis, I attempted to do a general tidying and cleaning of all datasets I would need for the different analyses. To start, this mostly entails reading in any CSV (Comma- Separated Values) files or web scraping an API (Application Programming Interface). Fortunately, the APIs I requested are usually pre- formatted and do not require much cleaning, as much as they would need to be formatted to fit the perspective I want to capture. On the other hand, most of the downloaded CSV datasets happen to have many missing values and are in need of a few extra steps of cleaning.\nOnce I have the raw data I want, Throughout this process, I am checking for missing values in the data and their locations in the datasets, to know if I will need to remove any informative rows or just marginal variables I will not be using.\n\n\nApple Music\nFor my Apple music dataset, I first dropped all missing values. I then melted down variables such as “Pts”, “Days”, and “Peak” to go along with the origin countries of the songs. I then removed anything that was not an integer from the (x?) column to represent the amount of times a song has been at that position. I then saved the cleaned version of my Apple dataset to a new CSV file.\n\n\nSpotify API\nMy Spotify dataset was already put together and void of issues, such as missing values, due to my previous R language request. Thus, the work required to clean the dataset did not require a large amount of time. My column names were generated by the API call, so I renamed all the variables to not include any excess characters. I did not delete any columns, just so my future feature selection is as accurate as possible. I then saved the cleaned dataset to another CSV file.\n\n\n\nCleaning Spotify Dataset in R\n\n\n\n\nNetflix\nFor my Netflix dataset, I renamed my column titles to a more precise description of what my features are meant to capture: the titles of the most- watched television shows and the amount of points each show has. I then deleted the first two rows, as they were repetitive displays of the column names. Last, I checked for any missing elements. Once I could see each column did not contain any missing values, I saved the cleaned dataset to a CSV file that can be found on my Github repository.\n\n\n\nCleaning Netflix Dataset in R\n\n\n\n\nGDP vs Self- Reported Life Satisfaction\nMy dataset for the GDP vs. Self- Reported Life Satisfaction was quite extensive. As the dataset contained data for countries over many years, I first read in my dataset and then subsetted the dataframe to only show information that pertains to the life satisfaction scores reported for the year 2022. My data source was not able to gather data on the gross domestic product (GDP) per capita for the year 2022 for each country, so I had to delete the column. The column was previously filled with GDP per capita information from the year 2017, so I resorted to gathering my own information on each country’s GDP per capita from the World Bank. As a side note, three countries’ GDP per capita indices were not provided on the World Bank’s site. So, I accessed data from Trading Economics to get the data points for Palestine and South Korea. Likewise, to get the GDP per capita information for Taiwan, I accessed the data information provided by Focus Economics.\nIn addition to dropping the previous “GDP per capita” column, I also eliminated the population, continent, and year (it was all for 2022) data. To precisely capture my dataset’s objective, I then renamed the Cantril Ladder Score column and gave a name to my updated GDP per capita column. Finally, I made sure there were no missing values and then saved the cleaned dataset to a new CSV file.\n\n\n\nCleaning Happiness Dataset in Python\n\n\n\n\nWikipedia API\nMy one other dataset source generated from an API call were my Wikipedia API datasets. For each of the top 5 Netflix shows I wanted to look at, I requested the information from Wikipedia’s website. For each show, I requested the most recent and available season to get access to the premise of the season. Once I was able to receive the premise, I generated a function to remove excess stopwords from the premise. After that, I recorded the sentiment analysis for each word that was left of the premise. Using the Sentiment Intensity Analyzer from the Natural Language Toolkit’s sentiment package, I could now see which words were considered negative, neutral, and positive and their associated sentiment compound scores. These variables were put in a dataframe once completed. I noticed that some dataframes considered “’s” as a word, so I had them removed as a final step before saving the dataset to a CSV file.\nOnce each television show premise was made into its own dataframe and saved to its own CSV file, I combined them. I did a vertical concatenation on the top five Netflix television shows as a final step before analysis commenced.\n\n\n\nCleaning Wikipedia Dataset in Python\n\n\nFor reference, I have posted my data cleaning process for one of my datasets– the Apple Music dataset. In the “Data Gathering” tab, you can see a general idea of what the original, raw dataset looks like. Additionally, the Apple Music dataset and all my other datasets are in the “websitedata” folder on my linked Github repository. Now, shown below, you can see a visual of my R cleaning inputs to form my dataset the way I need it for analysis. Right below that image, you can also see a visual of my cleaned dataset of top international rankings for Apple Music in 2023.\n\n\n\nCleaning Apple Dataset in R\n\n\n\n\n\nCleaned Apple Dataset\n\n\nSimilarly, as a second reference, right below this is a visual of my text sentiment analysis for Stranger Things, Season 3. The vectors display whether the analyzed word falls under the category of “neg” for negative, “neu” for neutral, and “pos” for positive. The last column labeled “compound” is the calculated net sentiment score. Right below that visual, is another image of vertical concatenation operation to combine all television shows."
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Intro:\nFor clustering, I will be analyzing my spotify dataset. As I want my labels to be removed and my analysis to be numerical, my feature dataset includes:\n\nDanceability\nEnergy: A metric of , loudness, and noise in a song.\nLoudness: A metric for the loudness; measured in decibels.\nSpeechiness: A metric for the presence of speech- adjacent aspects in a recording.\nAcousticness: A metric denoting how acoustic the song was.\nLiveness: A metric concerning the level of an audience presence in the recording.\nTempo: A metric of the estimated tempo of each song.\nAlbum Release Year: The release date of the specific album.\n\nI removed all of my variables containing strings and specifically, the Valence variable: - Artist Name: the creator of the particular song. - Valence: A metric of musical positivity. - Track Name: Another name for “song”. Shows the song name being analyzed. - Album Name: The specific album to which the song belongs.\nI removed Valence, due to it being the measure of overall positivity of a song. This feature in a dataset would represent what would influence listener choice, depending on their mood. Thus, I want to see how the Valence is distributed among Energy and Danceability– the variables of choice from my feature set.\n\n\nTheory:\nK- Means:\nK- Means Clustering revolves around partitioning data. The “k” in k- Means signifies the amount of averages that will simultaneously represent the amount of clusters. This clustering method has k amount of means as the “centroid” points that the data points will gravitate to. The data points will naturally move closer and closer to the centroid with the least amount of distance between them.\nThe best way to approach k- Means is to recompute the algorithm over and over. This means you assign points to nearest centroid, then repeat. The data points will move closer and closer to its nearest centroid. The distance between the data points and each centroid are computed by looking at the squared Euclidean distance between each point and the centroids.\nDBSCAN:\nDBSCAN stands for Density- Based Spacial Clustering of Applications with Noise. As the name suggests, DBSCAN is a clustering algorithm that clusters data points based on the densities they possess. Due to density being an indicator of how many data points are in a certain area, this technique is useful when wanting to look at high- density areas and, on the opposite end of the spectrum, outliers.\nDBSCAN holds a few advantages. It is one technique that does not need to be given an input of clusters beforehand, as the algorithm will find them on its own. Also, DBSCAN will not only form its own clusters, but its own shapes of clusters of data points. Due to DBSCAN being able to identify data points that are too far away from a potential cluster and its ability to discover clusters that do not have a linear relationship, the technique is very popular in unsupervised learning.\nHierarchical Clustering:\nHierarchical Clustering is a clustering technique that organizes data into growing partitions. There are two types of hierarchical clustering, one being Agglomerative, and the other being Division. Agglomerative Clustering starts off with the data points separated into as many partitions as possible and then are progressively combined with other clusters. Division Clustering is the opposite; the data is in one big cluster and is gradually separated into as many clusters as desired.\nLike DBSCAN, Hierarchical Clustering also does not need an input for the number of clusters, but finds them on their own. To see which data points have more similarity, for example in agglomerative clustering, you would examine which clusters are joined together earlier than others. This would mean their data points have less distance between them and are more likely to share behavior.\nModel Selection:\nModel selection is done through hyperparameter tuning (setting parameters before analysis). Two methods– the Elbow method and the Silhouette method– are used to find the optimal parameters to use. The Elbow method looks at the sum of squared distances between data points and tracks the expectedly exponential nature of the data as the number of clusters increases. This method is usually used for k- Means clustering.\nThe Silhouette method is used to determine how close clusters are to each other in similarity. The method assigns a score to clusters, -1 being the lowest score for inefficient clustering and +1 being the highest score for clearly separated clusters. The plot for the SIlhouette method presents a visual representation of how close each cluster is to the other.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport pandas as pd\n\n\n\n\nCode\nspotifydf = pd.read_csv(\"../websitedata/spotify_py.csv\")\n\nspotifydf = spotifydf.drop(columns={\"Unnamed: 0.1\", \"Unnamed: 0\", \"Artist Name\", \"Track Name\", \"Album Name\"}, axis=1)\nspotifyDF = spotifydf.drop(columns={\"Valence\"}) # Label (measures positivity) needs to be dropped\nspotifyDF.head\n\n\n&lt;bound method NDFrame.head of      Danceability  Energy  Loudness  Speechiness  Acousticness  Liveness  \\\n0           0.864   0.556    -7.683       0.1940      0.255000    0.1120   \n1           0.831   0.362    -6.085       0.2290      0.000156    0.1680   \n2           0.804   0.656    -5.723       0.0896      0.020700    0.1070   \n3           0.764   0.720    -6.494       0.2730      0.196000    0.1110   \n4           0.854   0.808    -5.958       0.1390      0.019400    0.1080   \n..            ...     ...       ...          ...           ...       ...   \n388         0.638   0.259   -10.706       0.0898      0.753000    0.2190   \n389         0.395   0.443    -9.720       0.1330      0.765000    0.0839   \n390         0.695   0.575    -6.334       0.1160      0.198000    0.0614   \n391         0.369   0.272   -10.497       0.0364      0.866000    0.1470   \n392         0.443   0.298   -12.181       0.0644      0.388000    0.1030   \n\n       Tempo  Album Release Year  \n0     99.974                2023  \n1    139.941                2023  \n2    150.014                2023  \n3     91.337                2023  \n4    110.203                2023  \n..       ...                 ...  \n388   88.485                2021  \n389  168.924                2021  \n390  163.929                2021  \n391  172.929                2021  \n392   77.253                2021  \n\n[393 rows x 8 columns]&gt;\n\n\n\n\nCode\nX = spotifyDF \ny = spotifydf[\"Valence\"] \n\nprint(X.shape, y.shape)\nX = np.ascontiguousarray(X)\n\n\n(393, 8) (393,)\n\n\n\n\nCode\n# UTILITY PLOTTING FUNCTION\ndef plot(X,y):\n    fig, ax = plt.subplots()\n    XPlot = ax.scatter(X[:,0], X[:,1],c=y, alpha=0.5) \n    ax.set(xlabel=\"Danceability\", ylabel=\"Energy\",\n    title=\"Cluster Data Analysis\")\n    ax.grid()\n    # fig.savefig(\"test.png\")\n    plt.colorbar(XPlot, label=\"Valence\")\n    plt.show()\n\nplot(X,y)\n\n\n\n\n\n\n\nCode\n# HYPERPARAMETER TUNING\nimport sklearn.cluster\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH)\n# AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE\ndef maximize_silhouette(X,algo=\"ag\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X)\n\n    # LOOP OVER HYPER-PARAM\n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.25*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue\n\n        if(i_print): print(param,sil_scores[-1])\n\n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")\n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\n\n\nCode\n# KMEANS\nopt_labels=maximize_silhouette(X,algo=\"kmeans\",nmax=15, i_plot=True)\nplot(X,opt_labels)\n\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\n\n\n\n\n\nCode\n# AGGLOMERATIVE CLUSTERING\nopt_labels=maximize_silhouette(X,algo=\"ag\",nmax=15, i_plot=True)\nplot(X,opt_labels)\n\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\n\n\n\n\n\nCode\n# DBSCAN\nopt_labels=maximize_silhouette(X,algo=\"dbscan\",nmax=15, i_plot=True)\nplot(X,opt_labels)\n\n\nOPTIMAL PARAMETER = 3.5\n\n\n\n\n\n\n\n\n\n\nFinal Analysis\n\n\nCode\n# KMEANS\nmodel = sklearn.cluster.KMeans(n_clusters=2).fit(X)\nlabels=model.predict(X)\nplot(X,labels)\n\n\n\n\n\n\n\nCode\n# AGGLOMERATIVE CLUSTERING\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=2).fit(X)\nlabels=model.labels_\nplot(X,labels)\n\n\n\n\n\n\n\nCode\n# DBSCAN \nmodel = sklearn.cluster.DBSCAN(eps=3.5).fit(X)\nlabels=model.labels_\nplot(X,labels)\n\n\n\n\n\n\n\nResults\nMy optimal parameter for the k- Means clustering and the Hierarchical Agglomerative clustering was 2 clusters, whereas it was 3.5 for DBSCAN. I believe DBSCAN did a better job at showing a relationship between the variables due to me getting a more detailed view of the clusters. As seen in the final DBSCAN, the lower the Energy and Danceability, the lower the Valence. However, it seems Valence tends to be highest at the middle point of Danceability and the higher end of Energy."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Ofure Udabor is a current first- year student in the Data Science and Analytics program at Georgetown University. She received her B.A. in Psychology and Economics from the University of Texas at Austin in 2022, where she her research focused on sociological effects of psychological and economic phenomena. During her undergraduate studies, she interned as a Data Analyst for an ecommerce brand and worked as a manager for a healthcare company while receiving academic accolades– such as University Honors– and merit scholarships– such as the Walter B. Smith Jr. Undergraduate Scholarship. Now as a graduate student, she hopes to gain a deep understanding of data science and how it can be leveraged in marketing to gain stronger insights from consumer behavior."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nGeorgetown University | Washington, DC\nM.S. in Data Science and Analytics | Aug 2023 - Present\nUniveristy of Texas at Austin | Austin, Texas\nB.A in Economics; B.A. in Psychology | Aug 2018 - May 2022"
  },
  {
    "objectID": "about.html#academic-interests",
    "href": "about.html#academic-interests",
    "title": "About",
    "section": "Academic Interests",
    "text": "Academic Interests\n\nNatural Language Processing\nMarketing Analytics\nDecision Analysis"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "To conclude this project, I have a summary of the various insights throughout the series of analyses:"
  },
  {
    "objectID": "conclusion.html#data-exploration",
    "href": "conclusion.html#data-exploration",
    "title": "Conclusion",
    "section": "Data Exploration:",
    "text": "Data Exploration:\n\nApple Music\nFor the Apple dataframe, the Peak had the least amount of standard deviation of -0.724705. Also, the variables yielded predominantly negative correlations, with the highest negative correlation being between Points and Chart Position. Meaning as the song moves up in ranking on the chart, the amount of points accumulated increases.\n\n\nSpotify API\nThe correlations were a fairly even mix of positive and negative correlation relationships. The highest positive correlation belongs to Energy and Loudness, meaning the louder the song, the more intensity captured as well. Also, acousticness and Energy appear to have the highest negative correlation. ### Netflix\nAmong the top television shows, the standard deviation was about 11,000 points. It can be assumed that the popularity of these shows has a wide amount of variation.\n\n\nGDP vs Self- Reported Life Satisfaction\nGross Domestic Product standard deviation was estimated to about 24,500 per capita, yet the report of life satisfaction has an estimated standard deviation of 1.14. Suggesting a large flexibility among life satisfaction. However, the correlation between the two variables is 0.695397.\n\n\nWikipedia API\nI produced word clouds to display the most used words for the top five- rated television shows. From observation, the Dahmer series is the only show that noticeably emphasizes negativity. This makes sense, as the story follows a serial killer.\nNaive Bayes: - I used the Multinomial Bayes Classifier on both my text and record datasets to discern how my self- reporting life satisfaction data is classified. Since my text and record data didn’t have many variables, the hyperparameter tuning was not as useful and the data suffered overfitting.\nClustering Analysis:\n\nMy optimal parameter for the k- Means clustering and the Hierarchical Agglomerative clustering was 2 clusters, whereas it was 3.5 for DBSCAN. I concluded DBSCAN was the favorable method. In the final DBSCAN, the lower the Energy and Danceability, the lower the Valence. However, the Valence was highest at the middle point of Danceability and the higher end of Energy.\n\nDimensionality Reduction:\n\nThe Principal Component Analysis yielded more variance as the songs reduced in(numbers increase for rankings) Country Rank and Chart Position. For t- SNE, the clusters had clearer separation as the perplexity reached 15. Suggesting the Apple Music dataset contains pretty distinct clusters in general.\n\nDecision Trees:\n\nThe decision tree produced 2 leaves, one with a value of 4.17 and another with a value of 7.287. My linear regression plot visualization shows the fit was not perfect, and fortunately the data did not suffer from overfitting.\n\nThe phenomena of streaming service preferences being linked to psychological standing holds several implications for readers. One being businesses need to progress in their depth of understanding the multiple facets of consumerism. With more and more platforms competing for customers– such as Spotify vs. Apple Music vs. Pandora and Netflix vs. Hulu vs. Peacock– persistent efforts to create gradually more adaptable recommendation systems is worth exploring.\nAnother implication being the benefits of consumers becoming more educated on how the entertainment they consume may affect them. As seen in the referenced works, there are a diverse array of effects on various moods to expect from different genres of media and their corresponding sentiments. It may hold significance for many consumers to educate themselves on the directions to take in the entertainment realm to enhance their well- being by encouraging an environment of consumption more conducive to their mental state."
  },
  {
    "objectID": "dataexploration.html",
    "href": "dataexploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "I have a pretty diverse group of data. With the four data sources I utilized, I have decided to spread my analysis among several datasets. To do this, I will first provide a summary of the data collected and then isolate certain datasets throughout the rest of the project.\n\n\nThe data types consist of numerical and categorical data. After cleaning, I am left with eight variables. These include: - Chart Position: The song’s position on the International Apple Music chart. - Artist and Title: The creator of the song and the song’s title - Position Duration: The length of time the song held that position. - Peak: The highest position the song has reached ever. - (x?): The amount of times the song has visited its current position. - Points: The amount of points the song has accumulated; justifying its position. - Country: The country in which the song originates. - Country Rank: The rank of the song in various countries.\n\n\n\nThe data types consist of predominantly numerical data. After cleaning, I am left with twelve variables. These include:\n\nArtist Name: the creator of the particular song.\nValence: A metric of musical positivity.\nDanceability\nEnergy: A metric of , loudness, and noise in a song.\nLoudness: A metric for the loudness; measured in decibels.\nSpeechiness: A metric for the presence of speech- adjacent aspects in a recording.\nAcousticness: A metric denoting how acoustic the song was.\nLiveness: A metric concerning the level of an audience presence in the recording.\nTempo: A metric of the estimated tempo of each song.\nTrack Name: Another name for “song”. Shows the song name being analyzed.\nAlbum Name: The specific album to which the song belongs.\nAlbum Release Year: The release date of the specific album.\n\n\n\n\nThe data types consist of predominantly numerical data. After cleaning, I am left with two variables. These include: The title of the television show on Netflix and the points accumulated by each show.\n\n10 points for the #1 television show\n9 points for the #2 television show . . .\n1 point for the #10 television show\n\n\n\n\nThe data types consist of predominantly numerical data. After cleaning, I am left with four variables. These include: - Entity: The country being analyzed. - Code: The official three- word abbreviation assigned to each country. - Score: The country average of life satisfaction. The scale used is the Cantril Ladder, which ranges from 0 to 10 (“0” being the lowest rating of life satisfaction and “10” being the highest rating of life satisfaction). - GDP: The Gross Domestic Product (GDP) per capita in international dollars (a normalized currency they used for comparison); this dataset borrowed statistics from 2017.\n\n\n\nThe data types consist of predominantly numerical data. After cleaning, I am left with four variables. These include:\n\n“Word”: The word from the premise being analyzed for sentiment.\n“neg”: The amount of negative sentiment detected in each word.\n“neu: The amount of neutral sentiment detected in each word.\n“pos”: The amount of positive sentiment detected in each word.\n“compound”: The net amount of sentiment detected in each word."
  },
  {
    "objectID": "dataexploration.html#data-understanding",
    "href": "dataexploration.html#data-understanding",
    "title": "Data Exploration",
    "section": "",
    "text": "I have a pretty diverse group of data. With the four data sources I utilized, I have decided to spread my analysis among several datasets. To do this, I will first provide a summary of the data collected and then isolate certain datasets throughout the rest of the project.\n\n\nThe data types consist of numerical and categorical data. After cleaning, I am left with eight variables. These include: - Chart Position: The song’s position on the International Apple Music chart. - Artist and Title: The creator of the song and the song’s title - Position Duration: The length of time the song held that position. - Peak: The highest position the song has reached ever. - (x?): The amount of times the song has visited its current position. - Points: The amount of points the song has accumulated; justifying its position. - Country: The country in which the song originates. - Country Rank: The rank of the song in various countries.\n\n\n\nThe data types consist of predominantly numerical data. After cleaning, I am left with twelve variables. These include:\n\nArtist Name: the creator of the particular song.\nValence: A metric of musical positivity.\nDanceability\nEnergy: A metric of , loudness, and noise in a song.\nLoudness: A metric for the loudness; measured in decibels.\nSpeechiness: A metric for the presence of speech- adjacent aspects in a recording.\nAcousticness: A metric denoting how acoustic the song was.\nLiveness: A metric concerning the level of an audience presence in the recording.\nTempo: A metric of the estimated tempo of each song.\nTrack Name: Another name for “song”. Shows the song name being analyzed.\nAlbum Name: The specific album to which the song belongs.\nAlbum Release Year: The release date of the specific album.\n\n\n\n\nThe data types consist of predominantly numerical data. After cleaning, I am left with two variables. These include: The title of the television show on Netflix and the points accumulated by each show.\n\n10 points for the #1 television show\n9 points for the #2 television show . . .\n1 point for the #10 television show\n\n\n\n\nThe data types consist of predominantly numerical data. After cleaning, I am left with four variables. These include: - Entity: The country being analyzed. - Code: The official three- word abbreviation assigned to each country. - Score: The country average of life satisfaction. The scale used is the Cantril Ladder, which ranges from 0 to 10 (“0” being the lowest rating of life satisfaction and “10” being the highest rating of life satisfaction). - GDP: The Gross Domestic Product (GDP) per capita in international dollars (a normalized currency they used for comparison); this dataset borrowed statistics from 2017.\n\n\n\nThe data types consist of predominantly numerical data. After cleaning, I am left with four variables. These include:\n\n“Word”: The word from the premise being analyzed for sentiment.\n“neg”: The amount of negative sentiment detected in each word.\n“neu: The amount of neutral sentiment detected in each word.\n“pos”: The amount of positive sentiment detected in each word.\n“compound”: The net amount of sentiment detected in each word."
  },
  {
    "objectID": "dataexploration.html#correlation-analysis",
    "href": "dataexploration.html#correlation-analysis",
    "title": "Data Exploration",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nExamine the correlations between variables using correlation matrices, heat-maps, or scatter plots. Identify which variables are positively, negatively, or not correlated, which can guide further analysis.\n\n\nCode\nprint(newAppledf.corr())\n\n\n                   Chart Position  Position Duration      Peak      (x?)  \\\nChart Position           1.000000           0.537127 -0.006928 -0.043663   \nPosition Duration        0.537127           1.000000 -0.234745  0.250226   \nPeak                    -0.006928          -0.234745  1.000000 -0.438067   \n(x?)                    -0.043663           0.250226 -0.438067  1.000000   \nPoints                  -0.788348          -0.543229  0.094603 -0.179048   \nCountry Rank             0.529540           0.329221 -0.048784  0.019253   \n\n                     Points  Country Rank  \nChart Position    -0.788348      0.529540  \nPosition Duration -0.543229      0.329221  \nPeak               0.094603     -0.048784  \n(x?)              -0.179048      0.019253  \nPoints             1.000000     -0.518561  \nCountry Rank      -0.518561      1.000000  \n\n\n\n\nCode\nprint(newspotdf.corr())\n\n\n                     Valence  Danceability    Energy  Loudness  Speechiness  \\\nValence             1.000000      0.246692  0.398002  0.307874     0.040146   \nDanceability        0.246692      1.000000 -0.041714  0.128736    -0.079113   \nEnergy              0.398002     -0.041714  1.000000  0.717260     0.179141   \nLoudness            0.307874      0.128736  0.717260  1.000000     0.059715   \nSpeechiness         0.040146     -0.079113  0.179141  0.059715     1.000000   \nAcousticness       -0.034774     -0.349418 -0.385829 -0.376252    -0.040905   \nLiveness           -0.041324     -0.066942  0.130201  0.082523     0.161238   \nTempo               0.050777     -0.063817  0.135110  0.086825     0.117097   \nAlbum Release Year  0.048786      0.154002 -0.074721  0.072098    -0.252975   \n\n                    Acousticness  Liveness     Tempo  Album Release Year  \nValence                -0.034774 -0.041324  0.050777            0.048786  \nDanceability           -0.349418 -0.066942 -0.063817            0.154002  \nEnergy                 -0.385829  0.130201  0.135110           -0.074721  \nLoudness               -0.376252  0.082523  0.086825            0.072098  \nSpeechiness            -0.040905  0.161238  0.117097           -0.252975  \nAcousticness            1.000000 -0.126337 -0.089189            0.037384  \nLiveness               -0.126337  1.000000  0.041149           -0.094607  \nTempo                  -0.089189  0.041149  1.000000            0.034514  \nAlbum Release Year      0.037384 -0.094607  0.034514            1.000000  \n\n\n\n\nCode\nprint(lifesat.corr())\n\n\n          Score       GDP\nScore  1.000000  0.695397\nGDP    0.695397  1.000000\n\n\n\nWikipedia Wordclouds\n\n\nCode\nimport wikipedia\n\nwikipedia.search(\"Stranger Things\", results=10, suggestion=False)\npage = wikipedia.WikipediaPage(\"Stranger Things (season 3)\").content\n# this gets text content from the page https://en.wikipedia.org/wiki/Stranger_Things_(season_3)\n\n# MODIFIED FROM \n# https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\nstrangerPage = \"In the summer of 1985, in Hawkins, the newly opened Starcourt Mall has become the center of attention in town, causing other stores to close their business due to its popularity. Jim Hopper disapproves of Mike Wheeler's relationship with his daughter Eleven, prompting him to intervene in their relationship, and leading to Eleven's friendship with Max Mayfield. Still recovering from Bob Newby's death, Joyce Byers considers moving out of Hawkins with her children. However, magnetic disruptions lead her to believe that the Upside Down has returned, so she enlists Hopper's help in uncovering the truth. While Mike and Lucas Sinclair attempt to repair his relationship with Eleven, Will Byers begins experiencing premonitions from the Upside Down, despite Eleven's closing of the original gate that led to his disappearance. He believes the Mind Flayer is still alive and back in Hawkins. As Nancy Wheeler and Jonathan Byers investigate the effects of the Mind Flayer's influence, Dustin Henderson, Steve Harrington, newcomer Robin Buckley, and Erica Sinclair begin investigating a potential Soviet infiltration of Hawkins. Meanwhile, Max's stepbrother Billy Hargrove is taken over by the Mind Flayer, forced to possess other citizens of Hawkins to fulfill the Mind Flayer's new plan. Eleven and her friends have no choice but to battle Billy, the other possessed victims, and destroy the Mind Flayer in its new and terrifying form, once and for all.\"\ngenerate_word_cloud(strangerPage)\n\n\n\n\n\n\n\nCode\nwikipedia.search(\"Manifest\", results=10, suggestion=False)\npage = wikipedia.WikipediaPage(\"Manifest (TV series)\").content\n# this gets text content from the page for Manifest the TV show\n\n# MODIFIED FROM \n# https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\npage = \"While traveling from Jamaica to New York City, Montego Air Flight 828 experiences a brief period of severe turbulence. When they land at Stewart International Airport in Newburgh, New York, the plane's 191 passengers and crew learn from NSA deputy director Robert 'Bobby' Vance that over five and a half years have passed while they were in the air, during which time they were presumed dead. As they rejoin society, the passengers must face the fact that their lives—and loved ones—are not the same as they were, while they also begin to experience guiding voices and visions representing events yet to occur, referred to as 'callings'.\"\n\ngenerate_word_cloud(page)\n\n\n\n\n\n\n\nCode\nwikipedia.search(\"Inventing Anna\", results=10, suggestion=False)\n#print(page)\npage = wikipedia.WikipediaPage(\"Inventing Anna\").content\n# this gets text content from the page for Inventing Anna the TV show\n\n# MODIFIED FROM \n# https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\npage = \"Inspired by the true story of a total fake. Under the assumed name Anna Delvey, Russian-born Anna Sorokin infiltrated New York's High society by convincing them she was a German socialite and an heiress to a massive fortune, all while scheming and scamming them out of millions.\"\n\ngenerate_word_cloud(page)\n\n\n\n\n\n\n\nCode\nwikipedia.search(\"Dahmer\", results=10, suggestion=False)\npage = wikipedia.WikipediaPage(\"Dahmer - Monster: The Jeffrey Dahmer Story\").content\n#print(page)\n# this gets text content from the page for Dahmer - Monster: The Jeffrey Dahmer Story the TV show\n\n# MODIFIED FROM \n# https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\npage = \"The series is about the life of Jeffrey Dahmer, and how he became one of the most notorious serial killers in America. His murders were executed in Bath Township, Ohio, West Allis, Wisconsin, and Milwaukee, Wisconsin between 1978 and 1991. The series dramatizes instances where Dahmer was nearly apprehended until his ultimate conviction and death. It also explores how police incompetence and apathy contributed to enabling his crimes.\"\n\ngenerate_word_cloud(page)\n\n\n\n\n\n\n\nCode\nwikipedia.search(\"Bridgerton\", results=10, suggestion=False)\npage = wikipedia.WikipediaPage(\"Bridgerton\").content\n#print(page)\n# this gets text content from the page for Bridgerton the TV show\n\n# MODIFIED FROM \n# https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\npage = \"Set against the backdrop of the Regency era, the eight close-knit siblings of the noble and powerful Bridgerton family – Anthony, Benedict, Colin, Daphne, Eloise, Francesca, Gregory and Hyacinth – navigate London high society in search of love, surrounded by friends and rivals alike.\"\n\ngenerate_word_cloud(page)"
  },
  {
    "objectID": "dataexploration.html#hypothesis-generation",
    "href": "dataexploration.html#hypothesis-generation",
    "title": "Data Exploration",
    "section": "Hypothesis Generation",
    "text": "Hypothesis Generation\n\nRefined Hypotheis and Questions:\n\nIs there a strong preference for negative content worldwide?\nIs the smaller than expected standard deviation for Self- Reported Life Satisfaction due to more positive music being favored there?\nIs there a strong preference for positive or neutral content worldwide?\nWhat songs are more likely to be preferred by which countries?\nDoes the self- reported life satisfaction data influence the the Peak of songs in the country?\nDoes more energy mean more positivity in songs?\nWould danceability be the biggest factor in how positive or negative a song is?\nAre the top five Netflix shows more negative?\nIs favored modern music more negative or positive?\nWhich country is most likely to choose a happy song over a sad song?\n\nI hypothesize the self- reported life satisfaction averages will have a significant influence on the sentiment of media and music topping rankings."
  },
  {
    "objectID": "dataexploration.html#methods-and-findings",
    "href": "dataexploration.html#methods-and-findings",
    "title": "Data Exploration",
    "section": "Methods and Findings",
    "text": "Methods and Findings\n\nApple Music\nFor the Apple dataframe, each variable seems to contain a very wide range of values. The variable with the least amount of deviation is the Peak, which has a standard deviation of -0.724705. This makes sense as it signifies the highest position attained by a song, which would be representative of a feat for most. Aside from that, the variables seem to mostly be negatively correlated, with the highest negative correlation being between Points and Chart Position. This means as the song get higher in chart position (the number of position decreases), the amount of points accumulated increases.\n\n\nSpotify API\nSpotify appears to have barely any deviation for most variables. The highest standard deviation belongs to Album Release Year. This makes sense, as the data frame contains information on artists’ song attributes across their years of their discography. The variables seem to have a very mixed outcome when looking at the amount of positive and negative correlation. The highest positive correlation belongs to Energy and Loudness, which yields a positive correlation of 0.717260. This would mean the louder the song, the more intensity captured as well. Acousticness and Energy appear to have the highest negative correlation, with a correlation of -0.385829. ### Netflix\nWhen looking at the amount of points each top- rated television show has, the least amount accumulated is 15,000 points while the maximum amount accumulated is about 64,000 points. With a standard deviation of about 11,000 points, it can be concluded that the popularity of these shows has a wide amount of variation.\n\n\nGDP vs Self- Reported Life Satisfaction\nInterestingly enough, although the Gross Domestic Product deviates by around 24,500 per capita, the report of life satisfaction has an estimated standard deviation of 1.14. This may suggest the extent to which the perspectives of people in a country are adjusted to their surroundings. Yet, the correlation between the two variables is 0.695397.\n\n\nWikipedia API\nTo display the findings of the Wikipedia text before specific analysis, I have presented word clouds below to examine the most used words for the top five- rated television shows. From observation, Stranger Things has more of an emphasis on the characters; Manifest focuses more on location and travelling; Inventing Anna looks more at the description of Anna Delvey the main character; Bridgerton mostly presents attributes of the family; while the Dahmer series is the only show that noticeably emphasizes negativity. This makes sense, as the story follows a serial killer."
  },
  {
    "objectID": "dataexploration.html#tools-and-software",
    "href": "dataexploration.html#tools-and-software",
    "title": "Data Exploration",
    "section": "Tools and Software",
    "text": "Tools and Software\nI will be primarily using Python for exploratory data analysis. This is due to the simplicity of the language making it easier for viewers to follow along with my data manipulation and the complexity of complementary sources, such as its libraries and frameworks. The libraries I am using include Pandas, Matplotlib, Seaborn, Numpy, scikit- learn, and MultinomialNB. While I am using Pandas to sort and manipulate my data, I will be using Matplotlib and Seaborn to visualize any findings and CountVectorizer to quantify useful text. I will be using Numpy to produce statistical information of my datasets that support my stance and visualizations. In addition to utilizing scikit- learn for producing general algorithms, I will implement MultinomialNB to generate my Naive Bayes algortihm."
  },
  {
    "objectID": "decisiontrees.html",
    "href": "decisiontrees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision Trees is a supervised learning method. This algorithm is constructed of rules that are meant to determine the possible outcome value of a target variable. Like a tree, there is a “root” node, which is simply the initial node of prediction. The nodes that follow, which are called “internal nodes”, are intermediary points of the decision process of the tree. Finally, The endpoints of the nodes are called “leaf nodes”, which house the predicted outcomes of the target variable.\nThere are few different ways for the tree to develop. The decision tree can have a binary split (the parent node is split into two “child nodes”) or a multi- way split (the parent node is split into more than two “child nodes”). Also, the decision tree can have either a continuous target variable or a categorical target variable. The tree with the continuous target variable (quantitative) is referred to as the “Regression Tree”. Whereas the decision tree with the categorical target variable (qualitative) is referred to as the “Classification Tree”.\nTo reduce the variance during such a decision process, Bagging can be used. Bagging, also known as, “Boostrap Aggregation”, is the process of taking many samples from one dataset, training on the desired number of the samples collected, and take an average of the predictions. This method is commonly used on decision trees in hopes of the outcome being more representative of the ground truth. This method, when done on multiple decision trees, is now establishing a “Random Forest”.\nAs decision trees represent one tree- structured process, Random Forests represent multiple. Instead of landing on a chosen outcome at the end of one true, the Random Forests algorithm implements “majority rules” by taking on the outcome that is predicted the most. This majority rules guideline is used for classification models in Random Forests. Whereas in a situation where the target variable is continuous, thus it is a regression model, the Random Forest takes the average of the bagging process to compute an outcome value.\nPlease refer to my “Regression” tab to view the decision tree algorithm applied to my chosen dataset."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Amanda Ofure Udabor’s Website GUID: au195",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "naivebayes.html",
    "href": "naivebayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Bayes Theorem is a mathematical theorem used for predicting a future outcome depending on a collected piece of evidence. More specifically, this theorem calculates “conditional probabilities”, which depicts the probability of an outcome based on a prior condition/ event. The formula for the theorem is:\n[P(A|B) = (P(B|A) (P(A))) / P(B)]\nIn this formula, to predict the probability of event A given event B, we multiply the probability of event B when event A occurs by the probability of event A. We then divide the product by the total probability of event B. With that in mind, Naive Bayes Classifier will be looking at how to classify a data set based on a target variable being given.\nThere are many variants under the umbrella of Naive Bayes. Included in this bunch are Complement Naive Bayes, Out- of- core Naive Bayes model- fitting, Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes. When speaking of Gaussian Naive Bayes, this is used for normal distribution expectancies in data. Whereas, Bernoulli Naive Bayes is used for binary variables when classifying. Likewise, Multinomial Naive Bayes is used for multiple variables.\nI will be using the Multinomial Bayes Classifier to examine how my text data is classified into positive, negative, and neutral sentiments. I will be using the Multinomial Bayes Classifier again to discern how my self- reporting life satisfaction data is classified.\n\n\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport shutil\n\n\n#OUTPUT FOLDER: START FRESH (DELETE OLD ONE IF EXISTS)\noutput_dir = \"../websitedata/naiveText\"\nif os.path.exists(output_dir) and os.path.isdir(output_dir):\n    shutil.rmtree(output_dir)\nos.mkdir(output_dir)\n\nteledf = pd.read_csv(\"../websitedata/television.csv\")\n\nteledf[\"sentiment\"] = teledf[\"compound\"].apply(lambda x: \"Pos\" if x &gt; 0.5 else \"Neg\" if x &lt; 0.0 else \"Neu\")\nteledf = teledf.dropna()\n#print(teledf.isna().sum())\nprint(teledf)\n\n     Unnamed: 0        Word  neg  neu  pos  compound sentiment\n0             0          in  0.0  1.0  0.0    0.0000       Neu\n1             1      summer  0.0  1.0  0.0    0.0000       Neu\n2             2       1985,  0.0  1.0  0.0    0.0000       Neu\n3             3    hawkins,  0.0  1.0  0.0    0.0000       Neu\n4             4       newly  0.0  1.0  0.0    0.0000       Neu\n..          ...         ...  ...  ...  ...       ...       ...\n305          26       love,  0.0  0.0  1.0    0.6369       Pos\n306          27  surrounded  0.0  1.0  0.0    0.0000       Neu\n307          28     friends  0.0  0.0  1.0    0.4767       Neu\n308          29      rivals  0.0  1.0  0.0    0.0000       Neu\n309          30      alike.  0.0  1.0  0.0    0.0000       Neu\n\n[310 rows x 7 columns]\n\n\n\n# use count vectorizer to retrieve one-hot encodings\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer(min_df = 2)\n\ncorpus = teledf[\"Word\"].to_list() \n\n# CLEAN UP\ntmp=[]\nfor word in corpus:\n    tmp1=word.replace(\",\",\"\")\n    tmp1=tmp1.replace(\".\",\"\")\n    #print(tmp1)\n    tmp.append(tmp1)\n    \ncorpus=tmp\nprint(corpus)\nprint(len(corpus))\n\n['in', 'summer', '1985', 'hawkins', 'newly', 'opened', 'starcourt', 'mall', 'become', 'center', 'attention', 'town', 'causing', 'stores', 'close', 'business', 'due', 'popularity', 'jim', 'hopper', 'disapproves', 'mike', 'wheeler', 'relationship', 'daughter', 'eleven', 'prompting', 'intervene', 'relationship', 'leading', 'eleven', 'friendship', 'max', 'mayfield', 'still', 'recovering', 'bob', 'newby', 'death', 'joyce', 'byers', 'considers', 'moving', 'hawkins', 'children', 'however', 'magnetic', 'disruptions', 'lead', 'believe', 'upside', 'down', 'returned', 'enlists', 'hopper', 'help', 'uncovering', 'truth', 'while', 'mike', 'lucas', 'sinclair', 'attempt', 'repair', 'relationship', 'eleven', 'will', 'byers', 'begins', 'experiencing', 'premonitions', 'upside', 'down', 'despite', 'eleven', 'closing', 'original', 'gate', 'led', 'disappearance', 'he', 'believes', 'mind', 'flayer', 'still', 'alive', 'back', 'hawkins', 'as', 'nancy', 'wheeler', 'jonathan', 'byers', 'investigate', 'effects', 'mind', 'flayer', 'influence', 'dustin', 'henderson', 'steve', 'harrington', 'newcomer', 'robin', 'buckley', 'erica', 'sinclair', 'begin', 'investigating', 'potential', 'soviet', 'infiltration', 'hawkins', 'meanwhile', 'max', 'stepbrother', 'billy', 'hargrove', 'taken', 'mind', 'flayer', 'forced', 'possess', 'citizens', 'hawkins', 'fulfill', 'mind', 'flayer', 'new', 'plan', 'eleven', 'friends', 'choice', 'battle', 'billy', 'possessed', 'victims', 'destroy', 'mind', 'flayer', 'new', 'terrifying', 'form', 'while', 'traveling', 'jamaica', 'new', 'york', 'city', 'montego', 'air', 'flight', '828', 'experiences', 'brief', 'period', 'severe', 'turbulence', 'when', 'land', 'stewart', 'international', 'airport', 'newburgh', 'new', 'york', 'plane', '191', 'passengers', 'crew', 'learn', 'nsa', 'deputy', 'director', 'robert', \"'bobby\", \"'\", 'vance', 'five', 'half', 'years', 'passed', 'air', 'time', 'presumed', 'dead', 'as', 'rejoin', 'society', 'passengers', 'must', 'face', 'fact', 'lives—and', 'loved', 'ones—are', 'also', 'begin', 'experience', 'guiding', 'voices', 'visions', 'representing', 'events', 'yet', 'occur', 'referred', \"'callings\", \"'\", 'inspired', 'true', 'story', 'total', 'fake', 'under', 'assumed', 'name', 'anna', 'delvey', 'russian-born', 'anna', 'sorokin', 'infiltrated', 'new', 'york', 'high', 'society', 'convincing', 'german', 'socialite', 'heiress', 'massive', 'fortune', 'scheming', 'scamming', 'millions', 'the', 'series', 'life', 'jeffrey', 'dahmer', 'became', 'one', 'notorious', 'serial', 'killers', 'america', 'his', 'murders', 'executed', 'bath', 'township', 'ohio', 'west', 'allis', 'wisconsin', 'milwaukee', 'wisconsin', '1978', '1991', 'the', 'series', 'dramatizes', 'instances', 'dahmer', 'nearly', 'apprehended', 'ultimate', 'conviction', 'death', 'it', 'also', 'explores', 'police', 'incompetence', 'apathy', 'contributed', 'enabling', 'crimes', 'set', 'backdrop', 'regency', 'era', 'eight', 'close-knit', 'siblings', 'noble', 'powerful', 'bridgerton', 'family', '–', 'anthony', 'benedict', 'colin', 'daphne', 'eloise', 'francesca', 'gregory', 'hyacinth', '–', 'navigate', 'london', 'high', 'society', 'search', 'love', 'surrounded', 'friends', 'rivals', 'alike']\n310\n\n\n\n# print(teledf[\"sentiment\"])\ny = np.array((teledf[\"sentiment\"]==\"Pos\").astype(int))\nprint(y)\nprint(y.shape)\n\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n(310,)\n\n\n\n# use count vectorizer to retrieve one-hot encodings\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer(min_df = 1)\nXs = count_vectorizer.fit_transform(corpus)   \n# print(Xs)\nprint(count_vectorizer.vocabulary_)\nx = np.array(Xs.todense())\nprint(x)\n\n{'in': 120, 'summer': 228, '1985': 2, 'hawkins': 110, 'newly': 172, 'opened': 180, 'starcourt': 221, 'mall': 151, 'become': 27, 'center': 44, 'attention': 21, 'town': 235, 'causing': 43, 'stores': 226, 'close': 49, 'business': 40, 'due': 73, 'popularity': 188, 'jim': 134, 'hopper': 117, 'disapproves': 69, 'mike': 156, 'wheeler': 250, 'relationship': 200, 'daughter': 60, 'eleven': 77, 'prompting': 195, 'intervene': 128, 'leading': 141, 'friendship': 101, 'max': 153, 'mayfield': 154, 'still': 225, 'recovering': 196, 'bob': 34, 'newby': 170, 'death': 62, 'joyce': 136, 'byers': 41, 'considers': 52, 'moving': 161, 'children': 45, 'however': 118, 'magnetic': 150, 'disruptions': 70, 'lead': 140, 'believe': 30, 'upside': 244, 'down': 71, 'returned': 203, 'enlists': 80, 'help': 113, 'uncovering': 242, 'truth': 239, 'while': 252, 'lucas': 149, 'sinclair': 216, 'attempt': 20, 'repair': 201, 'will': 253, 'begins': 29, 'experiencing': 87, 'premonitions': 193, 'despite': 65, 'closing': 50, 'original': 181, 'gate': 103, 'led': 143, 'disappearance': 68, 'he': 111, 'believes': 31, 'mind': 159, 'flayer': 94, 'alive': 8, 'back': 22, 'as': 18, 'nancy': 165, 'jonathan': 135, 'investigate': 129, 'effects': 75, 'influence': 124, 'dustin': 74, 'henderson': 114, 'steve': 223, 'harrington': 109, 'newcomer': 171, 'robin': 206, 'buckley': 39, 'erica': 82, 'begin': 28, 'investigating': 130, 'potential': 191, 'soviet': 220, 'infiltration': 123, 'meanwhile': 155, 'stepbrother': 222, 'billy': 33, 'hargrove': 108, 'taken': 230, 'forced': 96, 'possess': 189, 'citizens': 47, 'fulfill': 102, 'new': 168, 'plan': 185, 'friends': 100, 'choice': 46, 'battle': 25, 'possessed': 190, 'victims': 246, 'destroy': 66, 'terrifying': 231, 'form': 97, 'traveling': 237, 'jamaica': 132, 'york': 257, 'city': 48, 'montego': 160, 'air': 5, 'flight': 95, '828': 4, 'experiences': 86, 'brief': 38, 'period': 184, 'severe': 214, 'turbulence': 240, 'when': 251, 'land': 139, 'stewart': 224, 'international': 127, 'airport': 6, 'newburgh': 169, 'plane': 186, '191': 0, 'passengers': 183, 'crew': 56, 'learn': 142, 'nsa': 175, 'deputy': 64, 'director': 67, 'robert': 205, 'bobby': 35, 'vance': 245, 'five': 93, 'half': 107, 'years': 255, 'passed': 182, 'time': 233, 'presumed': 194, 'dead': 61, 'rejoin': 199, 'society': 218, 'must': 163, 'face': 89, 'fact': 90, 'lives': 145, 'and': 12, 'loved': 148, 'ones': 179, 'are': 17, 'also': 10, 'experience': 85, 'guiding': 106, 'voices': 248, 'visions': 247, 'representing': 202, 'events': 83, 'yet': 256, 'occur': 176, 'referred': 197, 'callings': 42, 'inspired': 125, 'true': 238, 'story': 227, 'total': 234, 'fake': 91, 'under': 243, 'assumed': 19, 'name': 164, 'anna': 13, 'delvey': 63, 'russian': 207, 'born': 36, 'sorokin': 219, 'infiltrated': 122, 'high': 115, 'convincing': 55, 'german': 104, 'socialite': 217, 'heiress': 112, 'massive': 152, 'fortune': 98, 'scheming': 209, 'scamming': 208, 'millions': 157, 'the': 232, 'series': 212, 'life': 144, 'jeffrey': 133, 'dahmer': 58, 'became': 26, 'one': 178, 'notorious': 174, 'serial': 211, 'killers': 137, 'america': 11, 'his': 116, 'murders': 162, 'executed': 84, 'bath': 24, 'township': 236, 'ohio': 177, 'west': 249, 'allis': 9, 'wisconsin': 254, 'milwaukee': 158, '1978': 1, '1991': 3, 'dramatizes': 72, 'instances': 126, 'nearly': 167, 'apprehended': 16, 'ultimate': 241, 'conviction': 54, 'it': 131, 'explores': 88, 'police': 187, 'incompetence': 121, 'apathy': 15, 'contributed': 53, 'enabling': 79, 'crimes': 57, 'set': 213, 'backdrop': 23, 'regency': 198, 'era': 81, 'eight': 76, 'knit': 138, 'siblings': 215, 'noble': 173, 'powerful': 192, 'bridgerton': 37, 'family': 92, 'anthony': 14, 'benedict': 32, 'colin': 51, 'daphne': 59, 'eloise': 78, 'francesca': 99, 'gregory': 105, 'hyacinth': 119, 'navigate': 166, 'london': 146, 'search': 210, 'love': 147, 'surrounded': 229, 'rivals': 204, 'alike': 7}\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 1 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\n\n\nimport random\n\nN=x.shape[0]; #print(\"N=\",N)\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nprint(train_index[0:10])\nprint(test_index[0:10])\nprint(N)\n\n[172, 100, 107, 197, 0, 190, 91, 185, 122, 63]\n[6, 105, 293, 59, 131, 161, 108, 66, 71, 38]\n310\n\n\n\n# FEATURE SELECTION\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(x, y, train_index, test_index, i_print=False):\n    if i_print:\n        print(x.shape, y.shape)\n\n    # Split the data using indices\n    x_train = x[train_index]\n    y_train = y[train_index].flatten()\n\n    x_test = x[test_index]\n    y_test = y[test_index].flatten()\n\n    # Initialize model\n    model = MultinomialNB()\n\n    # Train model\n    start = time.process_time()\n    model.fit(x_train, y_train)\n    time_train = time.process_time() - start\n\n    # Label predictions for training and test set\n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval = time.process_time() - start\n\n    acc_train = accuracy_score(y_train, yp_train) * 100\n    acc_test = accuracy_score(y_test, yp_test) * 100\n\n    if i_print:\n        print(acc_train, acc_test, time_train, time_eval)\n\n    return acc_train, acc_test, time_train, time_eval\n\n\n# Test\n(acc_train, acc_test, time_train, time_eval) = train_MNB_model(x, y, train_index, test_index, i_print=True)\n\n(310, 258) (310,)\n99.59677419354838 98.38709677419355 0.0 0.0\n\n\n\nx_var=np.var(x,axis=0)\nprint(np.min(x_var))\nprint(np.max(x_var))\n\n0.0032154006243496364\n0.015868886576482784\n\n\n\n##UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\n# UTILITY FUNCTION TO PLOT RESULTS\n\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\nfrom sklearn.feature_selection import VarianceThreshold\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=10\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,train_index, test_index, i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\nTHRESHOLD = 0.004621343507919986 33\nTHRESHOLD = 0.006027286391490336 33\nTHRESHOLD = 0.0074332292750606856 9\nTHRESHOLD = 0.008839172158631035 9\nTHRESHOLD = 0.010245115042201386 5\nTHRESHOLD = 0.011651057925771735 5\nTHRESHOLD = 0.013057000809342086 5\n\n\n\n# CHECK RESULTS \nsave_results(output_dir+\"/variance_threshold\")\nplot_results(output_dir+\"/variance_threshold\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport shutil\nimport requests\nimport json\nimport re\n#import pycountry\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\n\noutput_dir = output_dir = \"../websitedata/naiveOutput\"\nif os.path.exists(output_dir) and os.path.isdir(output_dir):\n    shutil.rmtree(output_dir)\nos.mkdir(output_dir)\n\nhappygdp = pd.read_csv(\"../websitedata/happy_py.csv\")\n\nhappygdp[\"GDP\"] = happygdp[\"GDP\"].str.replace(',', '').astype(float)\n\nhappygdp = happygdp.rename(columns={\"Unnamed: 0\": \"Country\"})\n\n# Score Categories: 0-4.0 = \"Low\"; 4.1 - 7.0 = \"Moderate\"; 7.1 & above = \"High\" \nhappygdp[\"Happiness\"] = pd.cut(happygdp[\"Score\"], bins=[0, 4.0, 7.0, 10], \n                     labels=[\"Low\", \"Moderate\", \"High\"])\n\nHappydf = happygdp.drop([\"Score\", \"Happiness\", \"Entity\", \"Code\"], axis=1)\n\nprint(Happydf)\n\n     Country      GDP\n0          1    363.7\n1        597   6802.8\n2        857   4273.9\n3       2133  13686.0\n4       2393   7014.2\n..       ...      ...\n132    56105   2255.2\n133    56816  15975.7\n134    57076   4163.5\n135    58181   1487.9\n136    58442   1267.0\n\n[137 rows x 2 columns]\n\n\n\nx = Happydf.to_numpy()\n\ny = happygdp[\"Happiness\"]\ny=np.array(y)\n\nprint(x.shape, y.shape)\n\n(137, 2) (137,)\n\n\n\nimport random\nN=x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nprint(train_index[0:10])\nprint(test_index[0:10])\n\n[118, 54, 0, 39, 68, 103, 93, 1, 61, 100]\n[85, 67, 87, 43, 2, 97, 108, 14, 25, 33]\n\n\n\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n#TEST\nprint(type(x),type(y))\nprint(x.shape,y.shape)\n(acc_train,acc_test,time_train,time_eval)=train_MNB_model(x,y,i_print=True)\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'numpy.ndarray'&gt;\n(137, 2) (137,)\n(137, 2) (137,)\n41.284403669724774 60.71428571428571 0.0 0.0\n\n\n\n# FEATURE SELECTION FOR RECORD DATA\n\nfrom sklearn.feature_selection import VarianceThreshold\n\n##UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n    \nx_var=np.var(x,axis=0)\nprint(np.min(x_var))\nprint(np.max(x_var))\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=30\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\n267473890.76700944\n595615971.6199687\nTHRESHOLD = 278789134.9343529 1\nTHRESHOLD = 290104379.10169625 1\nTHRESHOLD = 301419623.2690397 1\nTHRESHOLD = 312734867.4363831 1\nTHRESHOLD = 324050111.60372657 1\nTHRESHOLD = 335365355.77106994 1\nTHRESHOLD = 346680599.9384134 1\nTHRESHOLD = 357995844.1057568 1\nTHRESHOLD = 369311088.27310026 1\nTHRESHOLD = 380626332.44044363 1\nTHRESHOLD = 391941576.6077871 1\nTHRESHOLD = 403256820.7751305 1\nTHRESHOLD = 414572064.9424739 1\nTHRESHOLD = 425887309.1098173 1\nTHRESHOLD = 437202553.27716076 1\nTHRESHOLD = 448517797.44450414 1\nTHRESHOLD = 459833041.6118476 1\nTHRESHOLD = 471148285.779191 1\nTHRESHOLD = 482463529.9465344 1\nTHRESHOLD = 493778774.1138779 1\nTHRESHOLD = 505094018.2812213 1\nTHRESHOLD = 516409262.4485647 1\nTHRESHOLD = 527724506.61590815 1\nTHRESHOLD = 539039750.7832515 1\nTHRESHOLD = 550354994.9505949 1\nTHRESHOLD = 561670239.1179384 1\nTHRESHOLD = 572985483.2852818 1\n\n\n\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\n# CHECK RESULTS \nsave_results(output_dir+\"/variance_threshold\")\nplot_results(output_dir+\"/variance_threshold\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince my text and record data didn’t have many variables, the hyperparameter tuning was not as useful. Due to my data’s size, the data is also subject to overfitting."
  },
  {
    "objectID": "naivebayes.html#introduction-to-naive-bayes",
    "href": "naivebayes.html#introduction-to-naive-bayes",
    "title": "Naive Bayes",
    "section": "",
    "text": "Bayes Theorem is a mathematical theorem used for predicting a future outcome depending on a collected piece of evidence. More specifically, this theorem calculates “conditional probabilities”, which depicts the probability of an outcome based on a prior condition/ event. The formula for the theorem is:\n[P(A|B) = (P(B|A) (P(A))) / P(B)]\nIn this formula, to predict the probability of event A given event B, we multiply the probability of event B when event A occurs by the probability of event A. We then divide the product by the total probability of event B. With that in mind, Naive Bayes Classifier will be looking at how to classify a data set based on a target variable being given.\nThere are many variants under the umbrella of Naive Bayes. Included in this bunch are Complement Naive Bayes, Out- of- core Naive Bayes model- fitting, Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes. When speaking of Gaussian Naive Bayes, this is used for normal distribution expectancies in data. Whereas, Bernoulli Naive Bayes is used for binary variables when classifying. Likewise, Multinomial Naive Bayes is used for multiple variables.\nI will be using the Multinomial Bayes Classifier to examine how my text data is classified into positive, negative, and neutral sentiments. I will be using the Multinomial Bayes Classifier again to discern how my self- reporting life satisfaction data is classified.\n\n\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport shutil\n\n\n#OUTPUT FOLDER: START FRESH (DELETE OLD ONE IF EXISTS)\noutput_dir = \"../websitedata/naiveText\"\nif os.path.exists(output_dir) and os.path.isdir(output_dir):\n    shutil.rmtree(output_dir)\nos.mkdir(output_dir)\n\nteledf = pd.read_csv(\"../websitedata/television.csv\")\n\nteledf[\"sentiment\"] = teledf[\"compound\"].apply(lambda x: \"Pos\" if x &gt; 0.5 else \"Neg\" if x &lt; 0.0 else \"Neu\")\nteledf = teledf.dropna()\n#print(teledf.isna().sum())\nprint(teledf)\n\n     Unnamed: 0        Word  neg  neu  pos  compound sentiment\n0             0          in  0.0  1.0  0.0    0.0000       Neu\n1             1      summer  0.0  1.0  0.0    0.0000       Neu\n2             2       1985,  0.0  1.0  0.0    0.0000       Neu\n3             3    hawkins,  0.0  1.0  0.0    0.0000       Neu\n4             4       newly  0.0  1.0  0.0    0.0000       Neu\n..          ...         ...  ...  ...  ...       ...       ...\n305          26       love,  0.0  0.0  1.0    0.6369       Pos\n306          27  surrounded  0.0  1.0  0.0    0.0000       Neu\n307          28     friends  0.0  0.0  1.0    0.4767       Neu\n308          29      rivals  0.0  1.0  0.0    0.0000       Neu\n309          30      alike.  0.0  1.0  0.0    0.0000       Neu\n\n[310 rows x 7 columns]\n\n\n\n# use count vectorizer to retrieve one-hot encodings\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer(min_df = 2)\n\ncorpus = teledf[\"Word\"].to_list() \n\n# CLEAN UP\ntmp=[]\nfor word in corpus:\n    tmp1=word.replace(\",\",\"\")\n    tmp1=tmp1.replace(\".\",\"\")\n    #print(tmp1)\n    tmp.append(tmp1)\n    \ncorpus=tmp\nprint(corpus)\nprint(len(corpus))\n\n['in', 'summer', '1985', 'hawkins', 'newly', 'opened', 'starcourt', 'mall', 'become', 'center', 'attention', 'town', 'causing', 'stores', 'close', 'business', 'due', 'popularity', 'jim', 'hopper', 'disapproves', 'mike', 'wheeler', 'relationship', 'daughter', 'eleven', 'prompting', 'intervene', 'relationship', 'leading', 'eleven', 'friendship', 'max', 'mayfield', 'still', 'recovering', 'bob', 'newby', 'death', 'joyce', 'byers', 'considers', 'moving', 'hawkins', 'children', 'however', 'magnetic', 'disruptions', 'lead', 'believe', 'upside', 'down', 'returned', 'enlists', 'hopper', 'help', 'uncovering', 'truth', 'while', 'mike', 'lucas', 'sinclair', 'attempt', 'repair', 'relationship', 'eleven', 'will', 'byers', 'begins', 'experiencing', 'premonitions', 'upside', 'down', 'despite', 'eleven', 'closing', 'original', 'gate', 'led', 'disappearance', 'he', 'believes', 'mind', 'flayer', 'still', 'alive', 'back', 'hawkins', 'as', 'nancy', 'wheeler', 'jonathan', 'byers', 'investigate', 'effects', 'mind', 'flayer', 'influence', 'dustin', 'henderson', 'steve', 'harrington', 'newcomer', 'robin', 'buckley', 'erica', 'sinclair', 'begin', 'investigating', 'potential', 'soviet', 'infiltration', 'hawkins', 'meanwhile', 'max', 'stepbrother', 'billy', 'hargrove', 'taken', 'mind', 'flayer', 'forced', 'possess', 'citizens', 'hawkins', 'fulfill', 'mind', 'flayer', 'new', 'plan', 'eleven', 'friends', 'choice', 'battle', 'billy', 'possessed', 'victims', 'destroy', 'mind', 'flayer', 'new', 'terrifying', 'form', 'while', 'traveling', 'jamaica', 'new', 'york', 'city', 'montego', 'air', 'flight', '828', 'experiences', 'brief', 'period', 'severe', 'turbulence', 'when', 'land', 'stewart', 'international', 'airport', 'newburgh', 'new', 'york', 'plane', '191', 'passengers', 'crew', 'learn', 'nsa', 'deputy', 'director', 'robert', \"'bobby\", \"'\", 'vance', 'five', 'half', 'years', 'passed', 'air', 'time', 'presumed', 'dead', 'as', 'rejoin', 'society', 'passengers', 'must', 'face', 'fact', 'lives—and', 'loved', 'ones—are', 'also', 'begin', 'experience', 'guiding', 'voices', 'visions', 'representing', 'events', 'yet', 'occur', 'referred', \"'callings\", \"'\", 'inspired', 'true', 'story', 'total', 'fake', 'under', 'assumed', 'name', 'anna', 'delvey', 'russian-born', 'anna', 'sorokin', 'infiltrated', 'new', 'york', 'high', 'society', 'convincing', 'german', 'socialite', 'heiress', 'massive', 'fortune', 'scheming', 'scamming', 'millions', 'the', 'series', 'life', 'jeffrey', 'dahmer', 'became', 'one', 'notorious', 'serial', 'killers', 'america', 'his', 'murders', 'executed', 'bath', 'township', 'ohio', 'west', 'allis', 'wisconsin', 'milwaukee', 'wisconsin', '1978', '1991', 'the', 'series', 'dramatizes', 'instances', 'dahmer', 'nearly', 'apprehended', 'ultimate', 'conviction', 'death', 'it', 'also', 'explores', 'police', 'incompetence', 'apathy', 'contributed', 'enabling', 'crimes', 'set', 'backdrop', 'regency', 'era', 'eight', 'close-knit', 'siblings', 'noble', 'powerful', 'bridgerton', 'family', '–', 'anthony', 'benedict', 'colin', 'daphne', 'eloise', 'francesca', 'gregory', 'hyacinth', '–', 'navigate', 'london', 'high', 'society', 'search', 'love', 'surrounded', 'friends', 'rivals', 'alike']\n310\n\n\n\n# print(teledf[\"sentiment\"])\ny = np.array((teledf[\"sentiment\"]==\"Pos\").astype(int))\nprint(y)\nprint(y.shape)\n\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n(310,)\n\n\n\n# use count vectorizer to retrieve one-hot encodings\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer(min_df = 1)\nXs = count_vectorizer.fit_transform(corpus)   \n# print(Xs)\nprint(count_vectorizer.vocabulary_)\nx = np.array(Xs.todense())\nprint(x)\n\n{'in': 120, 'summer': 228, '1985': 2, 'hawkins': 110, 'newly': 172, 'opened': 180, 'starcourt': 221, 'mall': 151, 'become': 27, 'center': 44, 'attention': 21, 'town': 235, 'causing': 43, 'stores': 226, 'close': 49, 'business': 40, 'due': 73, 'popularity': 188, 'jim': 134, 'hopper': 117, 'disapproves': 69, 'mike': 156, 'wheeler': 250, 'relationship': 200, 'daughter': 60, 'eleven': 77, 'prompting': 195, 'intervene': 128, 'leading': 141, 'friendship': 101, 'max': 153, 'mayfield': 154, 'still': 225, 'recovering': 196, 'bob': 34, 'newby': 170, 'death': 62, 'joyce': 136, 'byers': 41, 'considers': 52, 'moving': 161, 'children': 45, 'however': 118, 'magnetic': 150, 'disruptions': 70, 'lead': 140, 'believe': 30, 'upside': 244, 'down': 71, 'returned': 203, 'enlists': 80, 'help': 113, 'uncovering': 242, 'truth': 239, 'while': 252, 'lucas': 149, 'sinclair': 216, 'attempt': 20, 'repair': 201, 'will': 253, 'begins': 29, 'experiencing': 87, 'premonitions': 193, 'despite': 65, 'closing': 50, 'original': 181, 'gate': 103, 'led': 143, 'disappearance': 68, 'he': 111, 'believes': 31, 'mind': 159, 'flayer': 94, 'alive': 8, 'back': 22, 'as': 18, 'nancy': 165, 'jonathan': 135, 'investigate': 129, 'effects': 75, 'influence': 124, 'dustin': 74, 'henderson': 114, 'steve': 223, 'harrington': 109, 'newcomer': 171, 'robin': 206, 'buckley': 39, 'erica': 82, 'begin': 28, 'investigating': 130, 'potential': 191, 'soviet': 220, 'infiltration': 123, 'meanwhile': 155, 'stepbrother': 222, 'billy': 33, 'hargrove': 108, 'taken': 230, 'forced': 96, 'possess': 189, 'citizens': 47, 'fulfill': 102, 'new': 168, 'plan': 185, 'friends': 100, 'choice': 46, 'battle': 25, 'possessed': 190, 'victims': 246, 'destroy': 66, 'terrifying': 231, 'form': 97, 'traveling': 237, 'jamaica': 132, 'york': 257, 'city': 48, 'montego': 160, 'air': 5, 'flight': 95, '828': 4, 'experiences': 86, 'brief': 38, 'period': 184, 'severe': 214, 'turbulence': 240, 'when': 251, 'land': 139, 'stewart': 224, 'international': 127, 'airport': 6, 'newburgh': 169, 'plane': 186, '191': 0, 'passengers': 183, 'crew': 56, 'learn': 142, 'nsa': 175, 'deputy': 64, 'director': 67, 'robert': 205, 'bobby': 35, 'vance': 245, 'five': 93, 'half': 107, 'years': 255, 'passed': 182, 'time': 233, 'presumed': 194, 'dead': 61, 'rejoin': 199, 'society': 218, 'must': 163, 'face': 89, 'fact': 90, 'lives': 145, 'and': 12, 'loved': 148, 'ones': 179, 'are': 17, 'also': 10, 'experience': 85, 'guiding': 106, 'voices': 248, 'visions': 247, 'representing': 202, 'events': 83, 'yet': 256, 'occur': 176, 'referred': 197, 'callings': 42, 'inspired': 125, 'true': 238, 'story': 227, 'total': 234, 'fake': 91, 'under': 243, 'assumed': 19, 'name': 164, 'anna': 13, 'delvey': 63, 'russian': 207, 'born': 36, 'sorokin': 219, 'infiltrated': 122, 'high': 115, 'convincing': 55, 'german': 104, 'socialite': 217, 'heiress': 112, 'massive': 152, 'fortune': 98, 'scheming': 209, 'scamming': 208, 'millions': 157, 'the': 232, 'series': 212, 'life': 144, 'jeffrey': 133, 'dahmer': 58, 'became': 26, 'one': 178, 'notorious': 174, 'serial': 211, 'killers': 137, 'america': 11, 'his': 116, 'murders': 162, 'executed': 84, 'bath': 24, 'township': 236, 'ohio': 177, 'west': 249, 'allis': 9, 'wisconsin': 254, 'milwaukee': 158, '1978': 1, '1991': 3, 'dramatizes': 72, 'instances': 126, 'nearly': 167, 'apprehended': 16, 'ultimate': 241, 'conviction': 54, 'it': 131, 'explores': 88, 'police': 187, 'incompetence': 121, 'apathy': 15, 'contributed': 53, 'enabling': 79, 'crimes': 57, 'set': 213, 'backdrop': 23, 'regency': 198, 'era': 81, 'eight': 76, 'knit': 138, 'siblings': 215, 'noble': 173, 'powerful': 192, 'bridgerton': 37, 'family': 92, 'anthony': 14, 'benedict': 32, 'colin': 51, 'daphne': 59, 'eloise': 78, 'francesca': 99, 'gregory': 105, 'hyacinth': 119, 'navigate': 166, 'london': 146, 'search': 210, 'love': 147, 'surrounded': 229, 'rivals': 204, 'alike': 7}\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 1 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\n\n\nimport random\n\nN=x.shape[0]; #print(\"N=\",N)\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nprint(train_index[0:10])\nprint(test_index[0:10])\nprint(N)\n\n[172, 100, 107, 197, 0, 190, 91, 185, 122, 63]\n[6, 105, 293, 59, 131, 161, 108, 66, 71, 38]\n310\n\n\n\n# FEATURE SELECTION\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(x, y, train_index, test_index, i_print=False):\n    if i_print:\n        print(x.shape, y.shape)\n\n    # Split the data using indices\n    x_train = x[train_index]\n    y_train = y[train_index].flatten()\n\n    x_test = x[test_index]\n    y_test = y[test_index].flatten()\n\n    # Initialize model\n    model = MultinomialNB()\n\n    # Train model\n    start = time.process_time()\n    model.fit(x_train, y_train)\n    time_train = time.process_time() - start\n\n    # Label predictions for training and test set\n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval = time.process_time() - start\n\n    acc_train = accuracy_score(y_train, yp_train) * 100\n    acc_test = accuracy_score(y_test, yp_test) * 100\n\n    if i_print:\n        print(acc_train, acc_test, time_train, time_eval)\n\n    return acc_train, acc_test, time_train, time_eval\n\n\n# Test\n(acc_train, acc_test, time_train, time_eval) = train_MNB_model(x, y, train_index, test_index, i_print=True)\n\n(310, 258) (310,)\n99.59677419354838 98.38709677419355 0.0 0.0\n\n\n\nx_var=np.var(x,axis=0)\nprint(np.min(x_var))\nprint(np.max(x_var))\n\n0.0032154006243496364\n0.015868886576482784\n\n\n\n##UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\n# UTILITY FUNCTION TO PLOT RESULTS\n\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\nfrom sklearn.feature_selection import VarianceThreshold\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=10\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,train_index, test_index, i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\nTHRESHOLD = 0.004621343507919986 33\nTHRESHOLD = 0.006027286391490336 33\nTHRESHOLD = 0.0074332292750606856 9\nTHRESHOLD = 0.008839172158631035 9\nTHRESHOLD = 0.010245115042201386 5\nTHRESHOLD = 0.011651057925771735 5\nTHRESHOLD = 0.013057000809342086 5\n\n\n\n# CHECK RESULTS \nsave_results(output_dir+\"/variance_threshold\")\nplot_results(output_dir+\"/variance_threshold\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport shutil\nimport requests\nimport json\nimport re\n#import pycountry\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\n\noutput_dir = output_dir = \"../websitedata/naiveOutput\"\nif os.path.exists(output_dir) and os.path.isdir(output_dir):\n    shutil.rmtree(output_dir)\nos.mkdir(output_dir)\n\nhappygdp = pd.read_csv(\"../websitedata/happy_py.csv\")\n\nhappygdp[\"GDP\"] = happygdp[\"GDP\"].str.replace(',', '').astype(float)\n\nhappygdp = happygdp.rename(columns={\"Unnamed: 0\": \"Country\"})\n\n# Score Categories: 0-4.0 = \"Low\"; 4.1 - 7.0 = \"Moderate\"; 7.1 & above = \"High\" \nhappygdp[\"Happiness\"] = pd.cut(happygdp[\"Score\"], bins=[0, 4.0, 7.0, 10], \n                     labels=[\"Low\", \"Moderate\", \"High\"])\n\nHappydf = happygdp.drop([\"Score\", \"Happiness\", \"Entity\", \"Code\"], axis=1)\n\nprint(Happydf)\n\n     Country      GDP\n0          1    363.7\n1        597   6802.8\n2        857   4273.9\n3       2133  13686.0\n4       2393   7014.2\n..       ...      ...\n132    56105   2255.2\n133    56816  15975.7\n134    57076   4163.5\n135    58181   1487.9\n136    58442   1267.0\n\n[137 rows x 2 columns]\n\n\n\nx = Happydf.to_numpy()\n\ny = happygdp[\"Happiness\"]\ny=np.array(y)\n\nprint(x.shape, y.shape)\n\n(137, 2) (137,)\n\n\n\nimport random\nN=x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nprint(train_index[0:10])\nprint(test_index[0:10])\n\n[118, 54, 0, 39, 68, 103, 93, 1, 61, 100]\n[85, 67, 87, 43, 2, 97, 108, 14, 25, 33]\n\n\n\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n#TEST\nprint(type(x),type(y))\nprint(x.shape,y.shape)\n(acc_train,acc_test,time_train,time_eval)=train_MNB_model(x,y,i_print=True)\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'numpy.ndarray'&gt;\n(137, 2) (137,)\n(137, 2) (137,)\n41.284403669724774 60.71428571428571 0.0 0.0\n\n\n\n# FEATURE SELECTION FOR RECORD DATA\n\nfrom sklearn.feature_selection import VarianceThreshold\n\n##UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n    \nx_var=np.var(x,axis=0)\nprint(np.min(x_var))\nprint(np.max(x_var))\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=30\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\n267473890.76700944\n595615971.6199687\nTHRESHOLD = 278789134.9343529 1\nTHRESHOLD = 290104379.10169625 1\nTHRESHOLD = 301419623.2690397 1\nTHRESHOLD = 312734867.4363831 1\nTHRESHOLD = 324050111.60372657 1\nTHRESHOLD = 335365355.77106994 1\nTHRESHOLD = 346680599.9384134 1\nTHRESHOLD = 357995844.1057568 1\nTHRESHOLD = 369311088.27310026 1\nTHRESHOLD = 380626332.44044363 1\nTHRESHOLD = 391941576.6077871 1\nTHRESHOLD = 403256820.7751305 1\nTHRESHOLD = 414572064.9424739 1\nTHRESHOLD = 425887309.1098173 1\nTHRESHOLD = 437202553.27716076 1\nTHRESHOLD = 448517797.44450414 1\nTHRESHOLD = 459833041.6118476 1\nTHRESHOLD = 471148285.779191 1\nTHRESHOLD = 482463529.9465344 1\nTHRESHOLD = 493778774.1138779 1\nTHRESHOLD = 505094018.2812213 1\nTHRESHOLD = 516409262.4485647 1\nTHRESHOLD = 527724506.61590815 1\nTHRESHOLD = 539039750.7832515 1\nTHRESHOLD = 550354994.9505949 1\nTHRESHOLD = 561670239.1179384 1\nTHRESHOLD = 572985483.2852818 1\n\n\n\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\n# CHECK RESULTS \nsave_results(output_dir+\"/variance_threshold\")\nplot_results(output_dir+\"/variance_threshold\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince my text and record data didn’t have many variables, the hyperparameter tuning was not as useful. Due to my data’s size, the data is also subject to overfitting."
  }
]